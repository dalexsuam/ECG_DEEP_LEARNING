{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmZXSdke_S2w",
        "outputId": "5a9cb806-155f-4342-ea8b-b293883f6027"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using device: cpu\n",
            "Dati tabulari puliti shape: (526, 15)\n",
            "ECG Segmenti shape: (526, 2500, 12)\n",
            "\n",
            "\n",
            "=======================================================\n",
            "INIZIO ESECUZIONE: Simple1DCNN (LR 1e-3)\n",
            "=======================================================\n",
            "\n",
            "--- FOLD 1/10 ---\n",
            "Early stopping attivato all'epoca 11!\n",
            "Fold 1 terminato. Miglior F1 all'epoca 2: 0.8090\n",
            "\n",
            "--- FOLD 2/10 ---\n",
            "Early stopping attivato all'epoca 18!\n",
            "Fold 2 terminato. Miglior F1 all'epoca 12: 0.8500\n",
            "\n",
            "--- FOLD 3/10 ---\n",
            "Early stopping attivato all'epoca 12!\n",
            "Fold 3 terminato. Miglior F1 all'epoca 3: 0.8182\n",
            "\n",
            "--- FOLD 4/10 ---\n",
            "Early stopping attivato all'epoca 19!\n",
            "Fold 4 terminato. Miglior F1 all'epoca 15: 0.8571\n",
            "\n",
            "--- FOLD 5/10 ---\n",
            "Early stopping attivato all'epoca 13!\n",
            "Fold 5 terminato. Miglior F1 all'epoca 3: 0.8372\n",
            "\n",
            "--- FOLD 6/10 ---\n",
            "Early stopping attivato all'epoca 14!\n",
            "Fold 6 terminato. Miglior F1 all'epoca 5: 0.8235\n",
            "\n",
            "--- FOLD 7/10 ---\n",
            "Early stopping attivato all'epoca 15!\n",
            "Fold 7 terminato. Miglior F1 all'epoca 10: 0.8611\n",
            "\n",
            "--- FOLD 8/10 ---\n",
            "Early stopping attivato all'epoca 13!\n",
            "Fold 8 terminato. Miglior F1 all'epoca 9: 0.8571\n",
            "\n",
            "--- FOLD 9/10 ---\n",
            "Early stopping attivato all'epoca 7!\n",
            "Fold 9 terminato. Miglior F1 all'epoca 2: 0.8182\n",
            "\n",
            "--- FOLD 10/10 ---\n",
            "Early stopping attivato all'epoca 10!\n",
            "Fold 10 terminato. Miglior F1 all'epoca 4: 0.8372\n",
            "\n",
            "RISULTATI MEDI Simple1DCNN (LR 1e-3):\n",
            "      Accuracy  F1 Score  Sensitivity (Recall)  Specificity     AUC\n",
            "mean    0.7396    0.8369                0.9694       0.2415  0.6771\n",
            "std     0.0443    0.0190                0.0480       0.2316  0.1246\n",
            "\n",
            "\n",
            "=======================================================\n",
            "INIZIO ESECUZIONE: Deep1DCNN (LR 1e-3)\n",
            "=======================================================\n",
            "\n",
            "--- FOLD 1/10 ---\n",
            "Early stopping attivato all'epoca 15!\n",
            "Fold 1 terminato. Miglior F1 all'epoca 2: 0.8182\n",
            "\n",
            "--- FOLD 2/10 ---\n",
            "Early stopping attivato all'epoca 9!\n",
            "Fold 2 terminato. Miglior F1 all'epoca 3: 0.8148\n",
            "\n",
            "--- FOLD 3/10 ---\n",
            "Early stopping attivato all'epoca 9!\n",
            "Fold 3 terminato. Miglior F1 all'epoca 8: 0.8500\n",
            "\n",
            "--- FOLD 4/10 ---\n",
            "Early stopping attivato all'epoca 7!\n",
            "Fold 4 terminato. Miglior F1 all'epoca 2: 0.8182\n",
            "\n",
            "--- FOLD 5/10 ---\n",
            "Early stopping attivato all'epoca 10!\n",
            "Fold 5 terminato. Miglior F1 all'epoca 6: 0.8267\n",
            "\n",
            "--- FOLD 6/10 ---\n",
            "Early stopping attivato all'epoca 17!\n",
            "Fold 6 terminato. Miglior F1 all'epoca 17: 0.8451\n",
            "\n",
            "--- FOLD 7/10 ---\n",
            "Early stopping attivato all'epoca 10!\n",
            "Fold 7 terminato. Miglior F1 all'epoca 4: 0.8675\n",
            "\n",
            "--- FOLD 8/10 ---\n",
            "Early stopping attivato all'epoca 16!\n",
            "Fold 8 terminato. Miglior F1 all'epoca 4: 0.8718\n",
            "\n",
            "--- FOLD 9/10 ---\n",
            "Early stopping attivato all'epoca 7!\n",
            "Fold 9 terminato. Miglior F1 all'epoca 6: 0.8276\n",
            "\n",
            "--- FOLD 10/10 ---\n",
            "Early stopping attivato all'epoca 9!\n",
            "Fold 10 terminato. Miglior F1 all'epoca 9: 0.8500\n",
            "\n",
            "RISULTATI MEDI Deep1DCNN (LR 1e-3):\n",
            "      Accuracy  F1 Score  Sensitivity (Recall)  Specificity     AUC\n",
            "mean    0.7511    0.8390                0.9444       0.3309  0.6882\n",
            "std     0.0415    0.0208                0.0600       0.2208  0.0776\n",
            "\n",
            "\n",
            "=======================================================\n",
            "INIZIO ESECUZIONE: Wide1DCNN (LR 5e-4)\n",
            "=======================================================\n",
            "\n",
            "--- FOLD 1/10 ---\n",
            "Early stopping attivato all'epoca 12!\n",
            "Fold 1 terminato. Miglior F1 all'epoca 3: 0.8140\n",
            "\n",
            "--- FOLD 2/10 ---\n",
            "Early stopping attivato all'epoca 19!\n",
            "Fold 2 terminato. Miglior F1 all'epoca 18: 0.8462\n",
            "\n",
            "--- FOLD 3/10 ---\n",
            "Early stopping attivato all'epoca 10!\n",
            "Fold 3 terminato. Miglior F1 all'epoca 3: 0.8182\n",
            "\n",
            "--- FOLD 4/10 ---\n",
            "Early stopping attivato all'epoca 16!\n",
            "Fold 4 terminato. Miglior F1 all'epoca 9: 0.8500\n",
            "\n",
            "--- FOLD 5/10 ---\n",
            "Early stopping attivato all'epoca 15!\n",
            "Fold 5 terminato. Miglior F1 all'epoca 3: 0.8276\n",
            "\n",
            "--- FOLD 6/10 ---\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import scipy\n",
        "from scipy.signal import butter, filtfilt, iirnotch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import scipy.io\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score, recall_score, accuracy_score, confusion_matrix, roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "\n",
        "# Importa librerie\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# Monta Drive e imposta Device\n",
        "drive.mount(\"/content/drive\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Impostazioni globali per la riproducibilità\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "else:\n",
        "    torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. PREPARAZIONE DATI E FUNZIONI DI FILTRAGGIO\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Funzioni di filtraggio ---\n",
        "def butter_bandpass(lowcut, highcut, fs, order=4):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    return b, a\n",
        "\n",
        "def apply_bandpass_filter(data, lowcut=1, highcut=40, fs=500, order=2):\n",
        "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "    return filtfilt(b, a, data)\n",
        "\n",
        "def notch_filter(data, freq=50, fs=500, quality_factor=30):\n",
        "    b, a = iirnotch(freq / (fs / 2), quality_factor)\n",
        "    return filtfilt(b, a, data)\n",
        "\n",
        "def extract_patient_id(filename):\n",
        "    return int(filename.split(\".\")[0])\n",
        "\n",
        "def segment_ecg(signal, segment_length=2500):\n",
        "    segments = np.zeros((signal.shape[0], segment_length, signal.shape[2]))\n",
        "    for i in range(signal.shape[0]):\n",
        "      start = 0 # Inizia dall'inizio\n",
        "      end = start + segment_length\n",
        "      segments[i, :, :] = signal[i, start:end, :]\n",
        "    return segments\n",
        "\n",
        "# --- Carica e Pre-processa Dati ---\n",
        "ECG_folder = \"/content/drive/MyDrive/ECG_Deep_Learning/1_batch_extracted\"\n",
        "ECG_folder_2batch = \"/content/drive/MyDrive/ECG_Deep_Learning/2_batch_extracted\"\n",
        "tabular_data = pd.read_excel(\"/content/drive/MyDrive/ECG_Deep_Learning/VALETUDO_database_1st_batch_en_all_info.xlsx\")\n",
        "tabular_data_2batch = pd.read_excel(\"/content/drive/MyDrive/ECG_Deep_Learning/VALETUDO_database_2nd_batch_en_all_info.xlsx\")\n",
        "\n",
        "ECGs_1 = [f for f in os.listdir(ECG_folder) if f.endswith(\".mat\")]\n",
        "ECGs_2 = [f for f in os.listdir(ECG_folder_2batch) if f.endswith(\".mat\")]\n",
        "ECGs_1.sort(key=extract_patient_id)\n",
        "ECGs_2.sort(key=extract_patient_id)\n",
        "\n",
        "signals_1 = np.empty((len(ECGs_1), 5000, 12))\n",
        "for index, ecg_path in enumerate(ECGs_1):\n",
        "    filepath = os.path.join(ECG_folder, ecg_path)\n",
        "    matdata = scipy.io.loadmat(filepath)\n",
        "    ecg = matdata['val']\n",
        "    for i in range(12):\n",
        "        ecg[:, i] = ecg[:, i] - np.mean(ecg[:, i])\n",
        "        ecg[:, i] = apply_bandpass_filter(ecg[:, i])\n",
        "        ecg[:, i] = notch_filter(ecg[:, i])\n",
        "    signals_1[index, :, :] = ecg\n",
        "\n",
        "signals_2 = np.empty((len(ECGs_2), 5000, 12))\n",
        "for index, ecg_path in enumerate(ECGs_2):\n",
        "    filepath = os.path.join(ECG_folder_2batch, ecg_path)\n",
        "    matdata = scipy.io.loadmat(filepath)\n",
        "    ecg = matdata['val']\n",
        "    for i in range(12):\n",
        "        ecg[:, i] = ecg[:, i] - np.mean(ecg[:, i])\n",
        "        ecg[:, i] = apply_bandpass_filter(ecg[:, i])\n",
        "        ecg[:, i] = notch_filter(ecg[:, i])\n",
        "    signals_2[index, :, :] = ecg\n",
        "\n",
        "# Concatenazione\n",
        "signals = np.concatenate([signals_1, signals_2], axis=0)\n",
        "tabular_data = pd.concat([\n",
        "    tabular_data.sort_values(by=\"ECG_patient_id\").reset_index(drop=True),\n",
        "    tabular_data_2batch.sort_values(by=\"ECG_patient_id\").reset_index(drop=True)\n",
        "], ignore_index=True)\n",
        "\n",
        "# Pulizia dati tabulari (rimozione colonne con NaN, per semplicità)\n",
        "missing_cols = tabular_data.columns[tabular_data.isnull().any()].tolist()\n",
        "tabular_data = tabular_data.drop(columns=missing_cols)\n",
        "print(f\"Dati tabulari puliti shape: {tabular_data.shape}\")\n",
        "\n",
        "# Segmentazione ECG (2500 campioni)\n",
        "ecg_segments = segment_ecg(signals, segment_length=2500)\n",
        "print(f\"ECG Segmenti shape: {ecg_segments.shape}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. DEFINIZIONE DEL DATASET E DELLE ARCHITETTURE (Simple, Deep, Wide)\n",
        "# ==============================================================================\n",
        "\n",
        "class ECGDataset(Dataset):\n",
        "    def __init__(self, signals, labels):\n",
        "        # Permuta i segnali in (N, Channels, Length) per Conv1d\n",
        "        if signals.shape[1] > signals.shape[2]:\n",
        "            self.signals = torch.tensor(signals, dtype=torch.float32).permute(0, 2, 1)\n",
        "        else:\n",
        "            self.signals = torch.tensor(signals, dtype=torch.float32)\n",
        "\n",
        "        self.labels = torch.tensor(labels.values, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.signals[idx], self.labels[idx]\n",
        "\n",
        "# --- 2.1. Simple1DCNN (Architettura Originale/Riferimento) ---\n",
        "class Simple1DCNN(nn.Module):\n",
        "    def __init__(self, num_leads=12):\n",
        "        super(Simple1DCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=num_leads, out_channels=32, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
        "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=5, padding=2)\n",
        "        self.bn3 = nn.BatchNorm1d(128)\n",
        "        self.pool3 = nn.MaxPool1d(kernel_size=2)\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc1 = nn.Linear(128, 64)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.gelu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool2(F.gelu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool3(F.gelu(self.bn3(self.conv3(x))))\n",
        "        x = self.global_pool(x).squeeze(-1)\n",
        "        x = F.gelu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "# --- 2.2. Deep1DCNN (Più Profonda) ---\n",
        "class Deep1DCNN(nn.Module):\n",
        "    def __init__(self, num_leads=12):\n",
        "        super(Deep1DCNN, self).__init__()\n",
        "        # Blocco 1-3 (come Simple)\n",
        "        self.conv1 = nn.Conv1d(in_channels=num_leads, out_channels=32, kernel_size=5, padding=2); self.bn1 = nn.BatchNorm1d(32); self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, padding=2); self.bn2 = nn.BatchNorm1d(64); self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
        "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=5, padding=2); self.bn3 = nn.BatchNorm1d(128); self.pool3 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        # Blocco 4 (NUOVO)\n",
        "        self.conv4 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=5, padding=2)\n",
        "        self.bn4 = nn.BatchNorm1d(256)\n",
        "        self.pool4 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc1 = nn.Linear(256, 64) # Input 256\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.gelu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool2(F.gelu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool3(F.gelu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool4(F.gelu(self.bn4(self.conv4(x)))) # Nuovo\n",
        "\n",
        "        x = self.global_pool(x).squeeze(-1)\n",
        "        x = F.gelu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "# --- 2.3. Wide1DCNN (Più Larga) ---\n",
        "class Wide1DCNN(nn.Module):\n",
        "    def __init__(self, num_leads=12):\n",
        "        super(Wide1DCNN, self).__init__()\n",
        "        # Blocco 1: 12 -> 64 (Era 32)\n",
        "        self.conv1 = nn.Conv1d(in_channels=num_leads, out_channels=64, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        # Blocco 2: 64 -> 128 (Era 64)\n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        # Blocco 3: 128 -> 256 (Era 128)\n",
        "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=5, padding=2)\n",
        "        self.bn3 = nn.BatchNorm1d(256)\n",
        "        self.pool3 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc1 = nn.Linear(256, 64) # Input 256\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.gelu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool2(F.gelu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool3(F.gelu(self.bn3(self.conv3(x))))\n",
        "\n",
        "        x = self.global_pool(x).squeeze(-1)\n",
        "        x = F.gelu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. FUNZIONE DI TRAINING E VALUTAZIONE\n",
        "# ==============================================================================\n",
        "\n",
        "def train_and_evaluate_model(ModelClass, ecg_data, tabular_data, learning_rate, num_epocs=50, batch_size=32, threshold=0.6, patience=5):\n",
        "    \"\"\"\n",
        "    Esegue la 10-Fold Cross-Validation per un dato modello e learning rate.\n",
        "    \"\"\"\n",
        "\n",
        "    # Inizializzazione della K-Fold\n",
        "    strat_kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "    # Liste per memorizzare le metriche di TUTTI i fold\n",
        "    metrics_all_folds = {\n",
        "        'f1': [], 'sensitivity': [], 'specificity': [], 'accuracy': [], 'auc': [], 'fpr': [], 'tpr': [], 'test_loss_history': [], 'train_loss_history': [],\n",
        "        'train_f1': [], 'train_sensitivity': [], 'train_specificity': [], 'train_accuracy': [], 'train_auc': []\n",
        "    }\n",
        "\n",
        "    # Inizializzazione Imputer e Scaler (solo una volta)\n",
        "    imputer = IterativeImputer(random_state=42)\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    for fold, (train_index, test_index) in enumerate(strat_kf.split(ecg_data, tabular_data['sport_ability'])):\n",
        "        print(f\"\\n--- FOLD {fold+1}/10 ---\")\n",
        "\n",
        "        # 1. Suddivisione dei Dati\n",
        "        ecg_train, ecg_test = ecg_data[train_index, :, :], ecg_data[test_index, :, :]\n",
        "        X_train, X_test = tabular_data.iloc[train_index,:], tabular_data.iloc[test_index,:]\n",
        "\n",
        "        Y_train = X_train['sport_ability']\n",
        "        Y_test = X_test['sport_ability']\n",
        "\n",
        "        # Prepara colonne tabulari da processare (NOTA: NON USATE NEL MODELLO, MA PROCESSATE PER COERENZA)\n",
        "        cols_to_drop = ['sport_ability', 'ECG_patient_id']\n",
        "        X_train_proc = X_train.drop(columns=[col for col in cols_to_drop if col in X_train.columns])\n",
        "        X_test_proc = X_test.drop(columns=[col for col in cols_to_drop if col in X_test.columns])\n",
        "\n",
        "        # Dinamicamente determina le colonne numeriche e categoriche presenti nel dataframe\n",
        "        all_available_cols_proc = X_train_proc.columns.tolist()\n",
        "\n",
        "        potential_numeric_cols = ['age_at_exam', 'height', 'weight', 'trainning_load']\n",
        "        potential_categorical_cols = ['sex', 'sport_classification'] # Add other categorical columns if any\n",
        "\n",
        "        numeric_cols = [col for col in potential_numeric_cols if col in all_available_cols_proc]\n",
        "        categorical_cols = [col for col in potential_categorical_cols if col in all_available_cols_proc]\n",
        "\n",
        "        # 2. Imputazione e Normalizzazione dei Dati Tabulari (Training)\n",
        "        X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train_proc), columns=X_train_proc.columns)\n",
        "        if numeric_cols:\n",
        "            X_train_imputed[numeric_cols] = scaler.fit_transform(X_train_imputed[numeric_cols])\n",
        "        # Imputazione e Normalizzazione dei Dati Tabulari (Test)\n",
        "        X_test_imputed = pd.DataFrame(imputer.transform(X_test_proc), columns=X_test_proc.columns)\n",
        "        if numeric_cols:\n",
        "            X_test_imputed[numeric_cols] = scaler.transform(X_test_imputed[numeric_cols])\n",
        "\n",
        "        # 3. Creazione Dataset e DataLoader\n",
        "        train_dataset = ECGDataset(ecg_train, Y_train)\n",
        "        test_dataset = ECGDataset(ecg_test, Y_test)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # 4. Inizializzazione Modello e Ottimizzatore\n",
        "        model = ModelClass(num_leads=12).to(device)\n",
        "        criterion = nn.BCELoss()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "        # Variabili Early Stopping e Loss History\n",
        "        best_test_loss = float('inf')\n",
        "        trigger_times = 0\n",
        "        current_train_loss_hist = []\n",
        "        current_test_loss_hist = []\n",
        "\n",
        "        # Variabili per trovare la miglior epoca del fold\n",
        "        best_epoch_metrics = {'f1': 0.0, 'epoch': 0, 'test_loss': 0.0}\n",
        "\n",
        "        # Inizializzazione metriche per epoca\n",
        "        epoch_metrics = {k: [] for k in metrics_all_folds.keys() if 'history' not in k and 'train_' not in k}\n",
        "        epoch_train_metrics = {k: [] for k in metrics_all_folds.keys() if 'train_' in k}\n",
        "\n",
        "        for epoch in range(num_epocs):\n",
        "\n",
        "            # --- TRAINING PHASE ---\n",
        "            model.train()\n",
        "            train_loss = 0.0\n",
        "            all_labels_train, all_preds_train, all_outputs_train = [], [], []\n",
        "\n",
        "            for signals_ecg, labels in train_loader:\n",
        "                signals_ecg, labels = signals_ecg.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(signals_ecg).squeeze() # Output: [Batch Size]\n",
        "\n",
        "                loss = criterion(outputs, labels.squeeze())\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                train_loss += loss.item()\n",
        "\n",
        "                predicted = (outputs > threshold).int()\n",
        "                all_labels_train.extend(labels.int().cpu().numpy())\n",
        "                all_preds_train.extend(predicted.cpu().numpy())\n",
        "\n",
        "            train_loss /= len(train_loader)\n",
        "            current_train_loss_hist.append(train_loss)\n",
        "\n",
        "            # Calcolo metriche di TRAIN\n",
        "            train_f1 = f1_score(all_labels_train, all_preds_train)\n",
        "            tn, fp, fn, tp = confusion_matrix(all_labels_train, all_preds_train).ravel()\n",
        "            train_sensitivity = tp / (tp + fn + 1e-8)\n",
        "            train_specificity = tn / (tn + fp + 1e-8)\n",
        "            train_accuracy = accuracy_score(all_labels_train, all_preds_train) * 100\n",
        "            train_auc = roc_auc_score(all_labels_train, all_preds_train)\n",
        "\n",
        "            # --- VALIDATION/TEST PHASE ---\n",
        "            model.eval()\n",
        "            test_loss = 0.0\n",
        "            all_labels_test, all_preds_test, all_outputs_test = [], [], []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for signals_ecg, labels in test_loader:\n",
        "                    signals_ecg, labels = signals_ecg.to(device), labels.to(device)\n",
        "                    outputs = model(signals_ecg).squeeze()\n",
        "\n",
        "                    loss = criterion(outputs, labels.squeeze())\n",
        "                    test_loss += loss.item()\n",
        "\n",
        "                    predicted = (outputs > threshold).int()\n",
        "                    all_labels_test.extend(labels.int().cpu().numpy())\n",
        "                    all_preds_test.extend(predicted.cpu().numpy())\n",
        "                    all_outputs_test.extend(outputs.cpu().numpy())\n",
        "\n",
        "            test_loss /= len(test_loader)\n",
        "            current_test_loss_hist.append(test_loss)\n",
        "\n",
        "            # Calcolo metriche di TEST\n",
        "            test_f1 = f1_score(all_labels_test, all_preds_test)\n",
        "            tn, fp, fn, tp = confusion_matrix(all_labels_test, all_preds_test).ravel()\n",
        "            test_sensitivity = tp / (tp + fn + 1e-8)\n",
        "            test_specificity = tn / (tn + fp + 1e-8)\n",
        "            test_accuracy = accuracy_score(all_labels_test, all_preds_test) * 100\n",
        "            test_auc = roc_auc_score(all_labels_test, all_outputs_test) # AUC usa gli output non binarizzati\n",
        "\n",
        "            # Curva ROC per il plot finale\n",
        "            fpr, tpr, _ = roc_curve(all_labels_test, all_outputs_test)\n",
        "\n",
        "            # Memorizza metriche dell'epoca\n",
        "            epoch_metrics['f1'].append(test_f1)\n",
        "            epoch_metrics['sensitivity'].append(test_sensitivity)\n",
        "            epoch_metrics['specificity'].append(test_specificity)\n",
        "            epoch_metrics['accuracy'].append(test_accuracy)\n",
        "            epoch_metrics['auc'].append(test_auc)\n",
        "            epoch_metrics['fpr'].append(fpr)\n",
        "            epoch_metrics['tpr'].append(tpr)\n",
        "\n",
        "            # Memorizza metriche di TRAIN per l'epoca\n",
        "            epoch_train_metrics['train_f1'].append(train_f1)\n",
        "            epoch_train_metrics['train_sensitivity'].append(train_sensitivity)\n",
        "            epoch_train_metrics['train_specificity'].append(train_specificity)\n",
        "            epoch_train_metrics['train_accuracy'].append(train_accuracy)\n",
        "            epoch_train_metrics['train_auc'].append(train_auc)\n",
        "\n",
        "            # Aggiorna la miglior epoca in base al F1-Score\n",
        "            if test_f1 > best_epoch_metrics['f1']:\n",
        "                best_epoch_metrics['f1'] = test_f1\n",
        "                best_epoch_metrics['epoch'] = epoch\n",
        "                best_epoch_metrics['test_loss'] = test_loss\n",
        "                # Salviamo i pesi del modello con il miglior F1\n",
        "                torch.save(model.state_dict(), f'best_model_fold_{fold}.pth')\n",
        "\n",
        "            # --- Early Stopping basato sulla Test Loss ---\n",
        "            if test_loss < best_test_loss:\n",
        "                 best_test_loss = test_loss\n",
        "                 trigger_times = 0\n",
        "            else:\n",
        "                 trigger_times += 1\n",
        "                 if trigger_times >= patience:\n",
        "                     print(f\"Early stopping attivato all'epoca {epoch+1}!\")\n",
        "                     # Ricarica i pesi migliori (basati sulla Loss, non sull'F1)\n",
        "                     # Per il confronto finale useremo l'epoca con F1 massimo.\n",
        "                     break\n",
        "\n",
        "        # 5. Fine Fold: Aggrega risultati della miglior epoca (basata su F1)\n",
        "        best_epoch_index = best_epoch_metrics['epoch']\n",
        "\n",
        "        print(f\"Fold {fold+1} terminato. Miglior F1 all'epoca {best_epoch_index+1}: {best_epoch_metrics['f1']:.4f}\")\n",
        "\n",
        "        # Aggiungi le metriche del FOLD (usando l'epoca con il miglior F1)\n",
        "        metrics_all_folds['f1'].append(epoch_metrics['f1'][best_epoch_index])\n",
        "        metrics_all_folds['sensitivity'].append(epoch_metrics['sensitivity'][best_epoch_index])\n",
        "        metrics_all_folds['specificity'].append(epoch_metrics['specificity'][best_epoch_index])\n",
        "        metrics_all_folds['accuracy'].append(epoch_metrics['accuracy'][best_epoch_index])\n",
        "        metrics_all_folds['auc'].append(epoch_metrics['auc'][best_epoch_index])\n",
        "        metrics_all_folds['fpr'].append(epoch_metrics['fpr'][best_epoch_index])\n",
        "        metrics_all_folds['tpr'].append(epoch_metrics['tpr'][best_epoch_index])\n",
        "\n",
        "        # Aggiungi le metriche di TRAIN per il confronto\n",
        "        metrics_all_folds['train_f1'].append(epoch_train_metrics['train_f1'][best_epoch_index])\n",
        "        metrics_all_folds['train_sensitivity'].append(epoch_train_metrics['train_sensitivity'][best_epoch_index])\n",
        "        metrics_all_folds['train_specificity'].append(epoch_train_metrics['train_specificity'][best_epoch_index])\n",
        "        metrics_all_folds['train_accuracy'].append(epoch_train_metrics['train_accuracy'][best_epoch_index])\n",
        "        metrics_all_folds['train_auc'].append(epoch_train_metrics['train_auc'][best_epoch_index])\n",
        "\n",
        "        # Aggiungi le loss history COMPLETE per i grafici\n",
        "        metrics_all_folds['train_loss_history'].append(current_train_loss_hist)\n",
        "        metrics_all_folds['test_loss_history'].append(current_test_loss_hist)\n",
        "\n",
        "    return metrics_all_folds\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. ESECUZIONE DEI TRE MODELLI\n",
        "# ==============================================================================\n",
        "\n",
        "# Dizionario per memorizzare i risultati finali di tutti i modelli\n",
        "final_results = {}\n",
        "configurations = [\n",
        "    {'name': 'Simple1DCNN (LR 1e-3)', 'model': Simple1DCNN, 'lr': 1e-3, 'threshold': 0.6},\n",
        "    {'name': 'Deep1DCNN (LR 1e-3)', 'model': Deep1DCNN, 'lr': 1e-3, 'threshold': 0.6},\n",
        "    {'name': 'Wide1DCNN (LR 5e-4)', 'model': Wide1DCNN, 'lr': 5e-4, 'threshold': 0.6},\n",
        "]\n",
        "\n",
        "for config in configurations:\n",
        "    print(f\"\\n\\n=======================================================\")\n",
        "    print(f\"INIZIO ESECUZIONE: {config['name']}\")\n",
        "    print(f\"=======================================================\")\n",
        "\n",
        "    # Esegui la 10-Fold CV\n",
        "    metrics = train_and_evaluate_model(\n",
        "        ModelClass=config['model'],\n",
        "        ecg_data=ecg_segments,\n",
        "        tabular_data=tabular_data,\n",
        "        learning_rate=config['lr'],\n",
        "        threshold=config['threshold']\n",
        "    )\n",
        "\n",
        "    # Prepara il DataFrame per il confronto\n",
        "    df_metrics = pd.DataFrame({\n",
        "        'Accuracy': np.array(metrics['accuracy']) / 100,\n",
        "        'F1 Score': metrics['f1'],\n",
        "        'Sensitivity (Recall)': metrics['sensitivity'],\n",
        "        'Specificity': metrics['specificity'],\n",
        "        'AUC': metrics['auc'],\n",
        "    })\n",
        "\n",
        "    # Memorizza i risultati\n",
        "    final_results[config['name']] = {\n",
        "        'df_results': df_metrics,\n",
        "        'metrics_raw': metrics,\n",
        "        'summary': df_metrics.describe().loc[['mean', 'std']]\n",
        "    }\n",
        "\n",
        "    print(f\"\\nRISULTATI MEDI {config['name']}:\")\n",
        "    print(final_results[config['name']]['summary'].round(4))\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. CONFRONTO E VISUALIZZAZIONE FINALE\n",
        "# ==============================================================================\n",
        "\n",
        "def plot_comparison(results_dict):\n",
        "    \"\"\"Genera boxplot per il confronto delle metriche tra tutti i modelli.\"\"\"\n",
        "\n",
        "    # Concatena tutti i DataFrame dei risultati\n",
        "    df_list = []\n",
        "    for name, data in results_dict.items():\n",
        "        df = data['df_results'].copy()\n",
        "        df['Configuration'] = name\n",
        "        df_list.append(df)\n",
        "\n",
        "    df_all = pd.concat(df_list, ignore_index=True)\n",
        "    df_melt = df_all.melt(id_vars='Configuration', var_name='Metric', value_name='Score')\n",
        "\n",
        "    # Visualizzazione Boxplot\n",
        "    plt.figure(figsize=(16, 7))\n",
        "    sns.boxplot(x='Metric', y='Score', hue='Configuration', data=df_melt, palette=\"Set2\")\n",
        "\n",
        "    plt.title('Confronto delle Performance (10-Fold CV) tra Architetture CNN')\n",
        "    plt.ylabel('Valore Metrica')\n",
        "    plt.xlabel('Metrica di Valutazione')\n",
        "    plt.legend(title='Modello', loc='lower right')\n",
        "    plt.grid(True, axis='y', alpha=0.3)\n",
        "    plt.ylim(0, 1.05)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    # Visualizzazione Curve ROC Medie\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "    for name, data in results_dict.items():\n",
        "        metrics = data['metrics_raw']\n",
        "        tprs = []\n",
        "        aucs = []\n",
        "\n",
        "        # Calcola la media delle curve ROC\n",
        "        for i in range(len(metrics['fpr'])):\n",
        "            interp_tpr = np.interp(mean_fpr, metrics['fpr'][i], metrics['tpr'][i])\n",
        "            interp_tpr[0] = 0.0\n",
        "            tprs.append(interp_tpr)\n",
        "            aucs.append(metrics['auc'][i])\n",
        "\n",
        "        mean_tpr = np.mean(tprs, axis=0)\n",
        "        mean_auc = np.mean(aucs)\n",
        "        std_auc = np.std(aucs)\n",
        "\n",
        "        plt.plot(\n",
        "            mean_fpr,\n",
        "            mean_tpr,\n",
        "            label=f'{name} (Mean AUC={mean_auc:.3f} \\u00b1 {std_auc:.3f})',\n",
        "            linewidth=2\n",
        "        )\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], color='black', linestyle='--', label='Chance')\n",
        "    plt.xlabel('False Positive Rate (FPR)')\n",
        "    plt.ylabel('True Positive Rate (TPR)')\n",
        "    plt.title('Curva ROC Media a Confronto (10-Fold CV)')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Esecuzione del Confronto Grafico\n",
        "plot_comparison(final_results)\n",
        "\n",
        "# Stampa il riepilogo finale\n",
        "print(\"\\n\\n#######################################################\")\n",
        "print(\"##### RIEPILOGO FINALE DELLE PERFORMANCE MEDIE #####\")\n",
        "print(\"#######################################################\")\n",
        "\n",
        "summary_list = []\n",
        "for name, data in final_results.items():\n",
        "    summary_df = data['summary'].copy().T\n",
        "    summary_df['Model'] = name\n",
        "    summary_list.append(summary_df)\n",
        "\n",
        "df_final_summary = pd.concat(summary_list).set_index(['Model', summary_list[0].index.name])\n",
        "df_final_summary.columns = ['Mean', 'Std']\n",
        "print(df_final_summary.round(4).to_markdown())"
      ]
    }
  ]
}