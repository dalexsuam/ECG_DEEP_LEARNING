{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfkbD_XShjRR"
   },
   "source": [
    "# Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:01:28.522922Z",
     "iopub.status.busy": "2025-11-29T13:01:28.522614Z",
     "iopub.status.idle": "2025-11-29T13:01:34.188999Z",
     "shell.execute_reply": "2025-11-29T13:01:34.188197Z",
     "shell.execute_reply.started": "2025-11-29T13:01:28.522897Z"
    },
    "id": "QqQWbG4rcbbu",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "# SUPPRESS WARININGs!!\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "PATH = \"/content/drive/MyDrive/PW02-Neuroengineering/\"\n",
    "\n",
    "\n",
    "from google.colab import drive\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.utils import resample\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "import math\n",
    "import scipy\n",
    "from scipy.signal import butter, filtfilt, iirnotch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import optuna\n",
    "\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, recall_score, accuracy_score, confusion_matrix, balanced_accuracy_score, roc_auc_score,  roc_curve, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab import drive\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:01:36.064854Z",
     "iopub.status.busy": "2025-11-29T13:01:36.064354Z",
     "iopub.status.idle": "2025-11-29T13:01:36.068168Z",
     "shell.execute_reply": "2025-11-29T13:01:36.067317Z",
     "shell.execute_reply.started": "2025-11-29T13:01:36.064829Z"
    },
    "id": "gj4ZPOEghcpL",
    "outputId": "19800132-6dda-403b-fe80-94990d7cbebd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#drive.mount(\"/content/drive\")\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbXRNbGth0mw"
   },
   "source": [
    "# Data Exploration and Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:01:36.451062Z",
     "iopub.status.busy": "2025-11-29T13:01:36.450486Z",
     "iopub.status.idle": "2025-11-29T13:01:36.456213Z",
     "shell.execute_reply": "2025-11-29T13:01:36.455434Z",
     "shell.execute_reply.started": "2025-11-29T13:01:36.451038Z"
    },
    "id": "42sid6n0ciGe",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#prepare functions for filtering\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=4):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def apply_bandpass_filter(data, lowcut=1, highcut=40, fs=500, order=2):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    return filtfilt(b, a, data)\n",
    "\n",
    "def notch_filter(data, freq=50, fs=500, quality_factor=30):      #  remove noise (50hz)\n",
    "    b, a = iirnotch(freq / (fs / 2), quality_factor)\n",
    "    return filtfilt(b, a, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:01:36.561862Z",
     "iopub.status.busy": "2025-11-29T13:01:36.561252Z",
     "iopub.status.idle": "2025-11-29T13:01:36.565056Z",
     "shell.execute_reply": "2025-11-29T13:01:36.564324Z",
     "shell.execute_reply.started": "2025-11-29T13:01:36.561841Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# only on kaggle\n",
    "\n",
    "PATH = \"/kaggle/input/test-ecg-pw02-1/drive-download-20251124T104021Z-1-001/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:01:36.717977Z",
     "iopub.status.busy": "2025-11-29T13:01:36.717187Z",
     "iopub.status.idle": "2025-11-29T13:01:37.377073Z",
     "shell.execute_reply": "2025-11-29T13:01:37.376321Z",
     "shell.execute_reply.started": "2025-11-29T13:01:36.717949Z"
    },
    "id": "qYB0dv2dctn_",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#import the data and filter the signals\n",
    "\n",
    "ECG_folder = f\"{PATH}1_batch_extracted\"\n",
    "ECG_folder_2batch = f\"{PATH}2_batch_extracted\"\n",
    "\n",
    "\n",
    "tabular_data = pd.read_excel(f\"{PATH}VALETUDO_database_1st_batch_en_all_info.xlsx\")\n",
    "tabular_data_2batch = pd.read_excel(f\"{PATH}VALETUDO_database_2nd_batch_en_all_info.xlsx\")\n",
    "\n",
    "# --- Load and filter both batches ---\n",
    "ECGs_1 = [f for f in os.listdir(ECG_folder) if f.endswith(\".mat\")]\n",
    "ECGs_2 = [f for f in os.listdir(ECG_folder_2batch) if f.endswith(\".mat\")]\n",
    "\n",
    "def extract_patient_id(filename):\n",
    "    return int(filename.split(\".\")[0])\n",
    "\n",
    "ECGs_1.sort(key=extract_patient_id)\n",
    "ECGs_2.sort(key=extract_patient_id)\n",
    "\n",
    "signals_1 = np.empty((len(ECGs_1), 5000, 12))    # empty 3d array   5000 --> time lenght / 12 --> leads\n",
    "signals_2 = np.empty((len(ECGs_2), 5000, 12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:01:37.378726Z",
     "iopub.status.busy": "2025-11-29T13:01:37.378241Z",
     "iopub.status.idle": "2025-11-29T13:01:46.062631Z",
     "shell.execute_reply": "2025-11-29T13:01:46.062021Z",
     "shell.execute_reply.started": "2025-11-29T13:01:37.378701Z"
    },
    "id": "z-c2GeCujX11",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for index, ecg_path in enumerate(ECGs_1):\n",
    "    filepath = os.path.join(ECG_folder, ecg_path)\n",
    "    matdata = scipy.io.loadmat(filepath)\n",
    "    ecg = matdata['val']\n",
    "    for i in range(12):\n",
    "        ecg[:, i] = ecg[:, i] - np.mean(ecg[:, i])    #signal centered in 0\n",
    "        ecg[:, i] = apply_bandpass_filter(ecg[:, i])  # filter\n",
    "        ecg[:, i] = notch_filter(ecg[:, i])           #filter noise\n",
    "    signals_1[index, :, :] = ecg\n",
    "\n",
    "# --- same ---\n",
    "\n",
    "for index, ecg_path in enumerate(ECGs_2):\n",
    "    filepath = os.path.join(ECG_folder_2batch, ecg_path)\n",
    "    matdata = scipy.io.loadmat(filepath)\n",
    "    ecg = matdata['val']\n",
    "    for i in range(12):\n",
    "        ecg[:, i] = ecg[:, i] - np.mean(ecg[:, i])\n",
    "        ecg[:, i] = apply_bandpass_filter(ecg[:, i])\n",
    "        ecg[:, i] = notch_filter(ecg[:, i])\n",
    "    signals_2[index, :, :] = ecg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:01:46.063602Z",
     "iopub.status.busy": "2025-11-29T13:01:46.063344Z",
     "iopub.status.idle": "2025-11-29T13:01:46.167908Z",
     "shell.execute_reply": "2025-11-29T13:01:46.166928Z",
     "shell.execute_reply.started": "2025-11-29T13:01:46.063577Z"
    },
    "id": "0gvtnDJyujwd",
    "outputId": "867c4aca-c358-499b-d3a9-a2579547ba4d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Concatenate signals and tabular data ---\n",
    "signals = np.concatenate([signals_1, signals_2], axis=0)\n",
    "tabular_data = pd.concat([\n",
    "    tabular_data.sort_values(by=\"ECG_patient_id\").reset_index(drop=True),\n",
    "    tabular_data_2batch.sort_values(by=\"ECG_patient_id\").reset_index(drop=True)\n",
    "], ignore_index=True)\n",
    "\n",
    "print(\"Combined signals shape:\", signals.shape)\n",
    "print(\"Combined tabular shape:\", tabular_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:01:46.170678Z",
     "iopub.status.busy": "2025-11-29T13:01:46.170313Z",
     "iopub.status.idle": "2025-11-29T13:01:46.177710Z",
     "shell.execute_reply": "2025-11-29T13:01:46.176888Z",
     "shell.execute_reply.started": "2025-11-29T13:01:46.170643Z"
    },
    "id": "rfN4Jd5DfrxL",
    "outputId": "fecc27d6-d17e-4cfb-cf82-5eb5e9260037",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"nb pos: {np.sum(tabular_data['sport_ability']==1)}\")\n",
    "print(f\"% pos: {np.sum(tabular_data['sport_ability']==1)/len(tabular_data['sport_ability'])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNtWP5fGxpyZ"
   },
   "source": [
    "dataset ~ sbilanciato, circa 70% classe 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:01:46.178803Z",
     "iopub.status.busy": "2025-11-29T13:01:46.178526Z",
     "iopub.status.idle": "2025-11-29T13:01:46.225509Z",
     "shell.execute_reply": "2025-11-29T13:01:46.224823Z",
     "shell.execute_reply.started": "2025-11-29T13:01:46.178783Z"
    },
    "id": "j8sUqnUNyI4e",
    "outputId": "2f07f759-7384-4713-aa69-24b5a83783fd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tabular_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:01:46.226413Z",
     "iopub.status.busy": "2025-11-29T13:01:46.226178Z",
     "iopub.status.idle": "2025-11-29T13:01:46.242905Z",
     "shell.execute_reply": "2025-11-29T13:01:46.242313Z",
     "shell.execute_reply.started": "2025-11-29T13:01:46.226396Z"
    },
    "id": "f0BTrM2FyXwp",
    "outputId": "2301fe19-9078-4d42-995c-549f08e98a6a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tabular_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2YoDS-Yyebw"
   },
   "source": [
    "do not need encoding, everything already numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:01:46.244003Z",
     "iopub.status.busy": "2025-11-29T13:01:46.243737Z",
     "iopub.status.idle": "2025-11-29T13:01:46.257823Z",
     "shell.execute_reply": "2025-11-29T13:01:46.257124Z",
     "shell.execute_reply.started": "2025-11-29T13:01:46.243987Z"
    },
    "id": "S587PBFy0UFo",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#tabular_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:01:46.258806Z",
     "iopub.status.busy": "2025-11-29T13:01:46.258497Z",
     "iopub.status.idle": "2025-11-29T13:01:46.280776Z",
     "shell.execute_reply": "2025-11-29T13:01:46.279997Z",
     "shell.execute_reply.started": "2025-11-29T13:01:46.258788Z"
    },
    "id": "6NnPQzN00KPJ",
    "outputId": "9717fbef-8423-4cb1-e191-cce6b56b6f89",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tabular_data = tabular_data.dropna(axis=1)\n",
    "tabular_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaT9HN9JftIN"
   },
   "source": [
    "rn dropped weight height training_load columns\n",
    "\n",
    "future test we can drop the rows (at least training load w 1 NA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MI1lhSp6ufC"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:01:46.282415Z",
     "iopub.status.busy": "2025-11-29T13:01:46.281558Z",
     "iopub.status.idle": "2025-11-29T13:01:46.386198Z",
     "shell.execute_reply": "2025-11-29T13:01:46.385565Z",
     "shell.execute_reply.started": "2025-11-29T13:01:46.282397Z"
    },
    "id": "EhZlYQ-g60D7",
    "outputId": "5b14c679-fd6c-4b01-b180-4c1e6fd98953",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "TARGET_COL = 'sport_ability'\n",
    "ID_COL = 'ECG_patient_id'\n",
    "\n",
    "patient_labels = tabular_data.groupby(ID_COL)[TARGET_COL].first()\n",
    "unique_patient_ids = patient_labels.index.values\n",
    "unique_patient_targets = patient_labels.values\n",
    "\n",
    "# 80% patient train\n",
    "train_ids, temp_ids, train_targets, temp_targets = train_test_split(\n",
    "    unique_patient_ids,\n",
    "    unique_patient_targets,\n",
    "    test_size=0.2,\n",
    "    stratify=unique_patient_targets,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# 10% patient val, 10% test\n",
    "val_ids, test_ids, val_targets, test_targets = train_test_split(\n",
    "    temp_ids,\n",
    "    temp_targets,\n",
    "    test_size=0.5,\n",
    "    stratify=temp_targets,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# mask\n",
    "train_mask = tabular_data[ID_COL].isin(train_ids)\n",
    "val_mask   = tabular_data[ID_COL].isin(val_ids)\n",
    "test_mask  = tabular_data[ID_COL].isin(test_ids)\n",
    "\n",
    "# --- TRAINING SET ---\n",
    "X_sig_train = signals[train_mask]\n",
    "X_tab_train = tabular_data[train_mask].drop(columns=[TARGET_COL])\n",
    "y_train     = tabular_data[train_mask][TARGET_COL]\n",
    "\n",
    "# --- VALIDATION SET ---\n",
    "X_sig_val   = signals[val_mask]\n",
    "X_tab_val   = tabular_data[val_mask].drop(columns=[TARGET_COL])\n",
    "y_val       = tabular_data[val_mask][TARGET_COL]\n",
    "\n",
    "# --- TEST SET ---\n",
    "X_sig_test  = signals[test_mask]\n",
    "X_tab_test  = tabular_data[test_mask].drop(columns=[TARGET_COL])\n",
    "y_test      = tabular_data[test_mask][TARGET_COL]\n",
    "\n",
    "\n",
    "# --- CHECK ---\n",
    "print(f\"Patients: {len(unique_patient_ids)}\")\n",
    "print(f\"Train IDs: {len(train_ids)}, Val IDs: {len(val_ids)}, Test IDs: {len(test_ids)}\")\n",
    "print(\"-\" * 30)\n",
    "print(f'Training set:   {X_sig_train.shape} {X_tab_train.shape} -> {y_train.shape}')\n",
    "print(f'Validation set: {X_sig_val.shape}   {X_tab_val.shape}   -> {y_val.shape}')\n",
    "print(f'Test set:       {X_sig_test.shape}  {X_tab_test.shape}  -> {y_test.shape}')\n",
    "\n",
    "assert set(tabular_data[train_mask][ID_COL]) & set(tabular_data[val_mask][ID_COL]) == set()\n",
    "print(\"Integrity OK --> no patient split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:01:46.388623Z",
     "iopub.status.busy": "2025-11-29T13:01:46.388350Z",
     "iopub.status.idle": "2025-11-29T13:01:46.570023Z",
     "shell.execute_reply": "2025-11-29T13:01:46.569417Z",
     "shell.execute_reply.started": "2025-11-29T13:01:46.388607Z"
    },
    "id": "MUclJJct60AA",
    "outputId": "cbd1dbcc-4ebb-4796-8d49-4be43e67b8d3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 3))\n",
    "\n",
    "sns.countplot(x=y_train, palette='tab10')\n",
    "\n",
    "plt.title(\"Train set sample distribution\")\n",
    "plt.xlabel(\"Sport Ability (0 -> NotAble // 1 --> Able)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAJe2uim_hAh"
   },
   "source": [
    "## Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:01:46.570893Z",
     "iopub.status.busy": "2025-11-29T13:01:46.570701Z",
     "iopub.status.idle": "2025-11-29T13:01:46.585020Z",
     "shell.execute_reply": "2025-11-29T13:01:46.584241Z",
     "shell.execute_reply.started": "2025-11-29T13:01:46.570879Z"
    },
    "id": "yUx-9AZL6z6M",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "COLS_TO_SCALE = ['age_at_exam']  # only normalize this column for tab data. add weight height if we dont delete them\n",
    "\n",
    "# normalize tabular data\n",
    "\n",
    "scaler_tab = MinMaxScaler()\n",
    "scaler_tab.fit(X_tab_train[COLS_TO_SCALE])\n",
    "\n",
    "X_tab_train.loc[:, COLS_TO_SCALE] = scaler_tab.transform(X_tab_train[COLS_TO_SCALE])\n",
    "X_tab_val.loc[:, COLS_TO_SCALE]   = scaler_tab.transform(X_tab_val[COLS_TO_SCALE])\n",
    "X_tab_test.loc[:, COLS_TO_SCALE]  = scaler_tab.transform(X_tab_test[COLS_TO_SCALE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:01:46.585882Z",
     "iopub.status.busy": "2025-11-29T13:01:46.585655Z",
     "iopub.status.idle": "2025-11-29T13:01:46.600797Z",
     "shell.execute_reply": "2025-11-29T13:01:46.600111Z",
     "shell.execute_reply.started": "2025-11-29T13:01:46.585859Z"
    },
    "id": "_sXgj0VWM1bM",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# func to normalize signals z-score\n",
    "def normalize_instance_wise(signals):\n",
    "\n",
    "    mean = np.mean(signals, axis=1, keepdims=True)\n",
    "    std = np.std(signals, axis=1, keepdims=True)\n",
    "\n",
    "    epsilon = 1e-8\n",
    "\n",
    "    return (signals - mean) / (std + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:01:46.601691Z",
     "iopub.status.busy": "2025-11-29T13:01:46.601488Z",
     "iopub.status.idle": "2025-11-29T13:01:47.086639Z",
     "shell.execute_reply": "2025-11-29T13:01:47.085932Z",
     "shell.execute_reply.started": "2025-11-29T13:01:46.601665Z"
    },
    "id": "OuDwaT1J6zrI",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_sig_train_norm = normalize_instance_wise(X_sig_train)\n",
    "X_sig_val_norm   = normalize_instance_wise(X_sig_val)\n",
    "X_sig_test_norm  = normalize_instance_wise(X_sig_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIy5s6b9nitN"
   },
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:01:47.087622Z",
     "iopub.status.busy": "2025-11-29T13:01:47.087384Z",
     "iopub.status.idle": "2025-11-29T13:01:47.092016Z",
     "shell.execute_reply": "2025-11-29T13:01:47.091295Z",
     "shell.execute_reply.started": "2025-11-29T13:01:47.087600Z"
    },
    "id": "HvomKLKfFZBe",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 2500\n",
    "STRIDE = 250        \n",
    "\n",
    "BATCH_SIZE = 32\n",
    "JITTER_STRENGTH = 0.1\n",
    "\n",
    "CHANNEL_MASK = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:01:47.093250Z",
     "iopub.status.busy": "2025-11-29T13:01:47.092942Z",
     "iopub.status.idle": "2025-11-29T13:01:47.110851Z",
     "shell.execute_reply": "2025-11-29T13:01:47.110115Z",
     "shell.execute_reply.started": "2025-11-29T13:01:47.093224Z"
    },
    "id": "NkSplsjnFY_K",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_sequences_numpy(signals, tabular, labels, window=2500, stride=1250):\n",
    "\n",
    "    X_sig_seq = []\n",
    "    X_tab_seq = []\n",
    "    y_seq = []\n",
    "\n",
    "    num_patients = signals.shape[0]\n",
    "    signal_len = signals.shape[1]\n",
    "\n",
    "    for i in range(num_patients):\n",
    "\n",
    "        curr_sig = signals[i]\n",
    "        curr_tab = tabular.iloc[i].values\n",
    "        curr_label = labels.iloc[i] if hasattr(labels, 'iloc') else labels[i]\n",
    "\n",
    "        idx = 0\n",
    "        while idx + window <= signal_len:\n",
    "            segment = curr_sig[idx : idx + window, :]\n",
    "\n",
    "            X_sig_seq.append(segment)\n",
    "            X_tab_seq.append(curr_tab)\n",
    "            y_seq.append(curr_label)\n",
    "\n",
    "            idx += stride\n",
    "\n",
    "    return np.array(X_sig_seq), np.array(X_tab_seq), np.array(y_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:01:47.111873Z",
     "iopub.status.busy": "2025-11-29T13:01:47.111616Z",
     "iopub.status.idle": "2025-11-29T13:01:47.247565Z",
     "shell.execute_reply": "2025-11-29T13:01:47.246850Z",
     "shell.execute_reply.started": "2025-11-29T13:01:47.111850Z"
    },
    "id": "zLXrithtFY8p",
    "outputId": "f61d845c-fa44-480b-b6b7-351cb10b4ce7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# training\n",
    "X_sig_train_seq, X_tab_train_seq, y_train_seq = build_sequences_numpy(\n",
    "    X_sig_train_norm, X_tab_train, y_train, window=WINDOW_SIZE, stride=1250\n",
    ")\n",
    "\n",
    "# Validation e Test (we dont augment data here)\n",
    "X_sig_val_seq, X_tab_val_seq, y_val_seq = build_sequences_numpy(\n",
    "    X_sig_val_norm, X_tab_val, y_val, window=WINDOW_SIZE, stride=WINDOW_SIZE\n",
    ")\n",
    "\n",
    "X_sig_test_seq, X_tab_test_seq, y_test_seq = build_sequences_numpy(\n",
    "    X_sig_test_norm, X_tab_test, y_test, window=WINDOW_SIZE, stride=WINDOW_SIZE\n",
    ")\n",
    "\n",
    "# --- check ---\n",
    "print(f\"Original Train Patients: {X_sig_train_norm.shape[0]}\")\n",
    "print(f\"Augmented Train Segments: {X_sig_train_seq.shape}\")\n",
    "print(f\"Augmented Tabular Shape: {X_tab_train_seq.shape}\")\n",
    "print(f\"Augmented Labels Shape: {y_train_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:01:47.248415Z",
     "iopub.status.busy": "2025-11-29T13:01:47.248224Z",
     "iopub.status.idle": "2025-11-29T13:01:47.256102Z",
     "shell.execute_reply": "2025-11-29T13:01:47.255437Z",
     "shell.execute_reply.started": "2025-11-29T13:01:47.248402Z"
    },
    "id": "EdxOFB5JFY5M",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ECGThreeBranchDataset(Dataset):\n",
    "    def __init__(self, signals, tabular, labels=None, is_train=False, JITTER_STRENGTH=0.05, CHANNEL_MASK=0.3):\n",
    "        \"\"\"\n",
    "        signals: (N, 2500, 12) -> splitted in (N, 6, 2500) and (N, 6, 2500)  --> to get 6 leads/6 leads\n",
    "        \"\"\"\n",
    "        self.signals = torch.tensor(signals, dtype=torch.float32).permute(0, 2, 1)\n",
    "        self.tabular = torch.tensor(tabular, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long) if labels is not None else None\n",
    "\n",
    "        self.is_train = is_train\n",
    "        self.jitter_strength = JITTER_STRENGTH\n",
    "        self.channel_mask_prob = CHANNEL_MASK\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.signals)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        full_sig = self.signals[idx].clone()\n",
    "\n",
    "        if self.is_train:\n",
    "\n",
    "            shift = torch.randint(low=-250, high=250, size=(1,)).item()   #time shifting\n",
    "            full_sig = torch.roll(full_sig, shifts=shift, dims=1)\n",
    "\n",
    "            if self.jitter_strength > 0:          # jitter -> put JITTER_STRENGHT = 0 to pass w/\n",
    "                noise = torch.randn_like(full_sig) * self.jitter_strength\n",
    "                full_sig = full_sig + noise\n",
    "\n",
    "            # Channel Masking   --> turn a signal to 0 to prevent model laziness\n",
    "            if self.channel_mask_prob > 0 and torch.rand(1) < self.channel_mask_prob:\n",
    "                mask_idx = torch.randint(0, 12, (1,)).item()\n",
    "                full_sig[mask_idx, :] = 0 \n",
    "                \n",
    "        \n",
    "        # we get 3 dataset --> model needs 3 branch\n",
    "        # branch 1: Limb Leads (first 6: I, II, III, aVR, aVL, aVF)\n",
    "        limb_sig = full_sig[:6, :]      # Shape: (6, 2500)\n",
    "\n",
    "        # Branch 2: Precordial Leads (last  6: V1-V6)\n",
    "        prec_sig = full_sig[6:, :]      # Shape: (6, 2500)\n",
    "\n",
    "        # Branch 3: Tabular\n",
    "        tab_data = self.tabular[idx]\n",
    "\n",
    "        if self.labels is not None:\n",
    "            return limb_sig, prec_sig, tab_data, self.labels[idx]\n",
    "        else:\n",
    "            return limb_sig, prec_sig, tab_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:01:52.041059Z",
     "iopub.status.busy": "2025-11-29T13:01:52.040367Z",
     "iopub.status.idle": "2025-11-29T13:01:52.045348Z",
     "shell.execute_reply": "2025-11-29T13:01:52.044603Z",
     "shell.execute_reply.started": "2025-11-29T13:01:52.041033Z"
    },
    "id": "MqoWg5dLFY0z",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def make_loader(ds, batch_size, shuffle, drop_last=False):\n",
    "\n",
    "    cpu_cores = os.cpu_count() or 2\n",
    "    num_workers = max(2, min(4, cpu_cores))\n",
    "\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
    "        prefetch_factor=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:01:52.189066Z",
     "iopub.status.busy": "2025-11-29T13:01:52.188490Z",
     "iopub.status.idle": "2025-11-29T13:01:52.345680Z",
     "shell.execute_reply": "2025-11-29T13:01:52.344990Z",
     "shell.execute_reply.started": "2025-11-29T13:01:52.189047Z"
    },
    "id": "9UJIx_JRFYuL",
    "outputId": "2b2e6a6a-67e9-482c-9ba1-59966150538c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# create train dataset\n",
    "train_ds = ECGThreeBranchDataset(\n",
    "    X_sig_train_seq, \n",
    "    X_tab_train_seq, \n",
    "    y_train_seq, \n",
    "    is_train=True,\n",
    "    JITTER_STRENGTH = JITTER_STRENGTH,   \n",
    "    CHANNEL_MASK = CHANNEL_MASK)\n",
    "# create val test dataset\n",
    "val_ds   = ECGThreeBranchDataset(X_sig_val_seq, X_tab_val_seq, y_val_seq, is_train=False)\n",
    "test_ds  = ECGThreeBranchDataset(X_sig_test_seq, X_tab_test_seq, y_test_seq, is_train=False)\n",
    "\n",
    "# create loaders\n",
    "train_loader = make_loader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "val_loader   = make_loader(val_ds,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = make_loader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"Dataloaders created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:01:52.705327Z",
     "iopub.status.busy": "2025-11-29T13:01:52.704551Z",
     "iopub.status.idle": "2025-11-29T13:01:53.109490Z",
     "shell.execute_reply": "2025-11-29T13:01:53.108616Z",
     "shell.execute_reply.started": "2025-11-29T13:01:52.705299Z"
    },
    "id": "g8JRBU_mvYbJ",
    "outputId": "70204d77-b044-4c25-ad7f-bd192edf1239",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- SANITY CHECK ---\n",
    "\n",
    "for limb_batch, prec_batch, tab_batch, label_batch in train_loader:\n",
    "\n",
    "    print(\"--- Batch Shapes ---\")\n",
    "    print(f\"Limb Leads Batch:        {limb_batch.shape}  -> (Batch, 6, Time)\")\n",
    "    print(f\"Precordial Leads Batch:  {prec_batch.shape}  -> (Batch, 6, Time)\")\n",
    "    print(f\"Tabular Batch:           {tab_batch.shape}   -> (Batch, Features)\")\n",
    "    print(f\"Labels Batch:            {label_batch.shape} -> (Batch)\")\n",
    "\n",
    "    assert limb_batch.shape[1] == 6, \"Error: Limb Leads should be 6\"\n",
    "    assert prec_batch.shape[1] == 6, \"Error: Precordial Leads should be 6\"\n",
    "    assert limb_batch.shape[2] == prec_batch.shape[2], \"Error time\"\n",
    "\n",
    "    break\n",
    "\n",
    "# --- Network Configuration Parameters ---\n",
    "n_timesteps = X_sig_train_seq.shape[1] # 2500\n",
    "total_channels = X_sig_train_seq.shape[2] # 12\n",
    "n_tab_feats = X_tab_train_seq.shape[1]\n",
    "n_classes   = len(np.unique(y_train_seq))\n",
    "\n",
    "print(\"\\n\\n--- Network Configuration ---\")\n",
    "print(f\"Total Input Channels: {total_channels} (Split into 6 Limb + 6 Precordial)\")\n",
    "print(f\"Input Tabular Features: {n_tab_feats}\")\n",
    "print(f\"Number of Classes:      {n_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:01:53.111074Z",
     "iopub.status.busy": "2025-11-29T13:01:53.110860Z",
     "iopub.status.idle": "2025-11-29T13:01:53.117041Z",
     "shell.execute_reply": "2025-11-29T13:01:53.116320Z",
     "shell.execute_reply.started": "2025-11-29T13:01:53.111053Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channel, reduction=4):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        return x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:01:53.118177Z",
     "iopub.status.busy": "2025-11-29T13:01:53.117917Z",
     "iopub.status.idle": "2025-11-29T13:01:53.134131Z",
     "shell.execute_reply": "2025-11-29T13:01:53.133552Z",
     "shell.execute_reply.started": "2025-11-29T13:01:53.118151Z"
    },
    "id": "-GghLdIhlNEl",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "convolution source:\n",
    "      Cardiologist-level arrhythmia detection and classification in ambulatory electrocardiograms using a deep neural network\n",
    "      Hannun, A. Y., Rajpurkar, P., Ng, A. Y., et al.\n",
    "      Nature Medicine (2019)\n",
    "      Link: Nature Medicine Article | https://arxiv.org/abs/1707.01836\n",
    "'''\n",
    "\n",
    "class MicroResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=7, stride=1):\n",
    "        super(MicroResNetBlock, self).__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, stride=1, padding=padding, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.se = SEBlock(out_channels, reduction=4)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride > 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.se(out)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axy8vItBxkMi"
   },
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:01:53.493918Z",
     "iopub.status.busy": "2025-11-29T13:01:53.493250Z",
     "iopub.status.idle": "2025-11-29T13:01:53.500704Z",
     "shell.execute_reply": "2025-11-29T13:01:53.499939Z",
     "shell.execute_reply.started": "2025-11-29T13:01:53.493894Z"
    },
    "id": "W0cNKgSMvYVg",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ThreeBranchSimpleCNN(nn.Module):\n",
    "    def __init__(self, n_tabular_features, n_classes, dropout=0.5, base_channels=8):\n",
    "        super(ThreeBranchSimpleCNN, self).__init__()\n",
    "\n",
    "        c1 = base_channels\n",
    "        c2 = base_channels * 2\n",
    "        c3 = base_channels * 4\n",
    "\n",
    "        def make_resnet_branch():\n",
    "            return nn.Sequential(\n",
    "                MicroResNetBlock(in_channels=6, out_channels=c1, kernel_size=15, stride=1), \n",
    "                nn.MaxPool1d(2),\n",
    "                \n",
    "                MicroResNetBlock(in_channels=c1, out_channels=c2, kernel_size=7, stride=2),\n",
    "                nn.MaxPool1d(2),\n",
    "                \n",
    "                MicroResNetBlock(in_channels=c2, out_channels=c3, kernel_size=5, stride=2),\n",
    "                                \n",
    "                nn.AdaptiveAvgPool1d(1) \n",
    "            )\n",
    "\n",
    "        self.branch_limb = make_resnet_branch()\n",
    "        self.branch_prec = make_resnet_branch()\n",
    "\n",
    "        self.branch_tab = nn.Sequential(\n",
    "            nn.Linear(n_tabular_features, c2), \n",
    "            nn.BatchNorm1d(c2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        fusion_dim = c3 + c3 + c2\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, c3 * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(c3 * 2, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, limb, prec, tab):\n",
    "        x_limb = self.branch_limb(limb).squeeze(-1)\n",
    "        x_prec = self.branch_prec(prec).squeeze(-1)\n",
    "        x_tab = self.branch_tab(tab)\n",
    "        combined = torch.cat([x_limb, x_prec, x_tab], dim=1)\n",
    "        return self.classifier(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:01:53.705303Z",
     "iopub.status.busy": "2025-11-29T13:01:53.704605Z",
     "iopub.status.idle": "2025-11-29T13:01:53.729872Z",
     "shell.execute_reply": "2025-11-29T13:01:53.729150Z",
     "shell.execute_reply.started": "2025-11-29T13:01:53.705280Z"
    },
    "id": "4ibRCujKvYNj",
    "outputId": "0e3b2081-39af-4701-bc29-727d7b71d8aa",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ---model instance ---\n",
    "\n",
    "N_TAB_FEATURES = X_tab_train_seq.shape[1]\n",
    "N_CLASSES = len(np.unique(y_train_seq))\n",
    "\n",
    "model = ThreeBranchSimpleCNN(\n",
    "    n_tabular_features=N_TAB_FEATURES,\n",
    "    n_classes=N_CLASSES\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device) \n",
    "\n",
    "print(model)  # to get all information of actual model\n",
    "print(f\"\\nModel loaded on: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CJO5WHdyQsH"
   },
   "source": [
    "# HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:01:54.095284Z",
     "iopub.status.busy": "2025-11-29T13:01:54.094792Z",
     "iopub.status.idle": "2025-11-29T13:01:54.103602Z",
     "shell.execute_reply": "2025-11-29T13:01:54.102709Z",
     "shell.execute_reply.started": "2025-11-29T13:01:54.095237Z"
    },
    "id": "SbUGMkmsx8n7",
    "outputId": "1b7ffcfb-7844-4e8a-d532-1fa36fabab41",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 500\n",
    "PATIENCE = 100\n",
    "model_dir = \"/content/\"\n",
    "VERBOSE = 1\n",
    "\n",
    "# --- Regularization ---\n",
    "DROPOUT_RATE = 0.5\n",
    "L1_LAMBDA = 0\n",
    "L2_LAMBDA = 5e-2\n",
    "LABEL_SMOOTHING = 0.15\n",
    "\n",
    "# class weights\n",
    "class_weights_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_seq),\n",
    "    y=y_train_seq\n",
    ")\n",
    "\n",
    "class_weights = torch.tensor(class_weights_array, dtype=torch.float32).to(device)\n",
    "\n",
    "print(f\"Wheights: {class_weights_array} for classes: {np.unique(y_train_seq)}\")\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=LABEL_SMOOTHING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWsY45thz7JW"
   },
   "source": [
    "## Functions & Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:01:54.511095Z",
     "iopub.status.busy": "2025-11-29T13:01:54.510467Z",
     "iopub.status.idle": "2025-11-29T13:01:54.518084Z",
     "shell.execute_reply": "2025-11-29T13:01:54.517367Z",
     "shell.execute_reply.started": "2025-11-29T13:01:54.511069Z"
    },
    "id": "JnO4STf3x8jY",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train function\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, scheduler, scaler, device, l1_lambda=0, l2_lambda=0):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    device_type = 'cuda' if device.type == 'cuda' else 'cpu'\n",
    "\n",
    "    for limb_in, prec_in, tab_in, targets in train_loader:\n",
    "        limb_in = limb_in.to(device)\n",
    "        prec_in = prec_in.to(device)\n",
    "        tab_in = tab_in.to(device)\n",
    "        targets = targets.to(device).long()\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.amp.autocast(device_type=device_type, enabled=(device.type=='cuda')):\n",
    "            logits = model(limb_in, prec_in, tab_in)\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "            # we dont need L1 rn, but here is the logic to make it work  -  L2 already managed by AdamW\n",
    "            if l1_lambda > 0:\n",
    "                l1_norm = sum(p.abs().sum() for p in model.parameters() if p.requires_grad)\n",
    "                loss += l1_lambda * l1_norm\n",
    "\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        running_loss += loss.item() * targets.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_f1 = f1_score(np.concatenate(all_targets), np.concatenate(all_preds), average='weighted')\n",
    "    return epoch_loss, epoch_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:01:54.719873Z",
     "iopub.status.busy": "2025-11-29T13:01:54.719164Z",
     "iopub.status.idle": "2025-11-29T13:01:54.725665Z",
     "shell.execute_reply": "2025-11-29T13:01:54.724988Z",
     "shell.execute_reply.started": "2025-11-29T13:01:54.719849Z"
    },
    "id": "U2mcU30jx8er",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# validate function\n",
    "def validate_one_epoch(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    device_type = 'cuda' if device.type == 'cuda' else 'cpu'\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for limb_in, prec_in, tab_in, targets in val_loader:\n",
    "            limb_in = limb_in.to(device)\n",
    "            prec_in = prec_in.to(device)\n",
    "            tab_in = tab_in.to(device)\n",
    "            targets = targets.to(device).long()\n",
    "\n",
    "            with torch.amp.autocast(device_type=device_type, enabled=(device.type=='cuda')):\n",
    "                logits = model(limb_in, prec_in, tab_in)\n",
    "                loss = criterion(logits, targets)\n",
    "\n",
    "            running_loss += loss.item() * targets.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    epoch_f1 = f1_score(np.concatenate(all_targets), np.concatenate(all_preds), average='weighted')\n",
    "    return epoch_loss, epoch_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:01:54.920185Z",
     "iopub.status.busy": "2025-11-29T13:01:54.919565Z",
     "iopub.status.idle": "2025-11-29T13:01:54.929054Z",
     "shell.execute_reply": "2025-11-29T13:01:54.928454Z",
     "shell.execute_reply.started": "2025-11-29T13:01:54.920161Z"
    },
    "id": "tMpO7B8dx8aT",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scheduler, scaler, device,\n",
    "        l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n",
    "        restore_best_weights=True, verbose=1, experiment_name=\"best_model\"):\n",
    "\n",
    "    training_history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_f1': [], 'val_f1': []\n",
    "    }\n",
    "\n",
    "    if patience > 0:\n",
    "        patience_counter = 0\n",
    "        best_metric = float('-inf') if mode == 'max' else float('inf')\n",
    "        best_epoch = 0\n",
    "\n",
    "    print(f\"Start Training: {epochs} epochs on {device}... \\n\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        train_loss, train_f1 = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, scheduler, scaler, device, l1_lambda, l2_lambda\n",
    "        )\n",
    "\n",
    "        val_loss, val_f1 = validate_one_epoch(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "\n",
    "        training_history['train_loss'].append(train_loss)\n",
    "        training_history['val_loss'].append(val_loss)\n",
    "        training_history['train_f1'].append(train_f1)\n",
    "        training_history['val_f1'].append(val_f1)\n",
    "\n",
    "        # print\n",
    "        if verbose > 0 and (epoch % verbose == 0 or epoch == 1):\n",
    "            current_lr = scheduler.get_last_lr()[0] if scheduler else optimizer.param_groups[0]['lr']\n",
    "            print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f} F1: {train_f1:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} F1: {val_f1:.4f} | \"\n",
    "                  f\"LR: {current_lr:.6f}\")\n",
    "\n",
    "        # early stopping\n",
    "        if patience > 0:\n",
    "            current_metric = val_f1 if evaluation_metric == \"val_f1\" else val_loss\n",
    "\n",
    "            if mode == 'max':\n",
    "                is_improvement = current_metric > best_metric\n",
    "            else:\n",
    "                is_improvement = current_metric < best_metric\n",
    "\n",
    "            if is_improvement:\n",
    "                best_metric = current_metric\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(), f\"{model_dir}/{experiment_name}.pt\")\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"\\nEarly stopping triggered after {epoch} epochs.\")\n",
    "                    break\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTraining completed in {total_time/60:.2f} minutes.\")\n",
    "\n",
    "    if restore_best_weights and patience > 0:\n",
    "        model.load_state_dict(torch.load(f\"{model_dir}/{experiment_name}.pt\", map_location=device))\n",
    "        print(f\"Recovered best model: Epoch {best_epoch} with {evaluation_metric}: {best_metric:.4f}\")\n",
    "\n",
    "    return model, training_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3_X0oaV0BoI"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:01:55.328162Z",
     "iopub.status.busy": "2025-11-29T13:01:55.327463Z",
     "iopub.status.idle": "2025-11-29T13:01:55.331388Z",
     "shell.execute_reply": "2025-11-29T13:01:55.330629Z",
     "shell.execute_reply.started": "2025-11-29T13:01:55.328135Z"
    },
    "id": "hgopyhkXx8V5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "best_model = None\n",
    "best_performance = float('-inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:01:55.532404Z",
     "iopub.status.busy": "2025-11-29T13:01:55.531700Z",
     "iopub.status.idle": "2025-11-29T13:01:58.110899Z",
     "shell.execute_reply": "2025-11-29T13:01:58.110307Z",
     "shell.execute_reply.started": "2025-11-29T13:01:55.532377Z"
    },
    "id": "NjloSgEBx8RL",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Create model and display architecture ---\n",
    "experiment_name = \"ThreeBranchSimpleCNN  -  -  -  OPTUNA\"\n",
    "\n",
    "# Recuperiamo le dimensioni dai dati pronti\n",
    "n_tabular_features = X_tab_train_seq.shape[1] # Features tabellari\n",
    "n_classes = len(np.unique(y_train_seq))       # Classi\n",
    "\n",
    "\n",
    "# optimezer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)\n",
    "\n",
    "# scheduler --> test maybe w ReduceOnPlateu\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=2e-4,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    epochs=EPOCHS,\n",
    "    pct_start=0.15,          # 15% epochs LR up, 85% down\n",
    "    anneal_strategy='cos'\n",
    ")\n",
    "\n",
    "try:\n",
    "    scaler = torch.amp.GradScaler(device='cuda', enabled=(device.type == 'cuda'))\n",
    "except AttributeError:\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(device.type == 'cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:04:05.261738Z",
     "iopub.status.busy": "2025-11-29T13:04:05.261394Z",
     "iopub.status.idle": "2025-11-29T13:04:05.272359Z",
     "shell.execute_reply": "2025-11-29T13:04:05.271518Z",
     "shell.execute_reply.started": "2025-11-29T13:04:05.261702Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    # Optimizer & Scheduler\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 5e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-4, 5e-2, log=True)\n",
    "    pct_start = trial.suggest_float(\"pct_start\", 0.1, 0.5) \n",
    "    \n",
    "    # Architecture\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.2, 0.6)\n",
    "    base_channels = trial.suggest_categorical(\"base_channels\", [8, 16, 32]) \n",
    "    \n",
    "    # Training\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128])\n",
    "    \n",
    "    # Augmentation\n",
    "    jitter = trial.suggest_float(\"jitter\", 0.0, 0.2)\n",
    "    mask_prob = trial.suggest_float(\"mask_prob\", 0.0, 0.5)\n",
    "    \n",
    "    # Loss\n",
    "    label_smoothing = trial.suggest_float(\"label_smoothing\", 0.0, 0.2)\n",
    "\n",
    "    # ==========================================\n",
    "    # 2. DATASET & LOADER\n",
    "    # ==========================================\n",
    "    train_ds = ECGThreeBranchDataset(\n",
    "        X_sig_train_seq, X_tab_train_seq, y_train_seq, \n",
    "        is_train=True, \n",
    "        JITTER_STRENGTH=jitter,  \n",
    "        CHANNEL_MASK=mask_prob    \n",
    "    )\n",
    "    val_ds = ECGThreeBranchDataset(X_sig_val_seq, X_tab_val_seq, y_val_seq, is_train=False)\n",
    "\n",
    "    train_loader_opt = make_loader(train_ds, batch_size, shuffle=True, drop_last=True)\n",
    "    val_loader_opt = make_loader(val_ds, batch_size, shuffle=False)\n",
    "\n",
    "    # ==========================================\n",
    "    # 3. MODEL \n",
    "    # ==========================================\n",
    "    model = ThreeBranchSimpleCNN(\n",
    "        n_tabular_features=N_TAB_FEATURES, \n",
    "        n_classes=2,\n",
    "        dropout=dropout,\n",
    "        base_channels=base_channels \n",
    "    ).to(device)\n",
    "\n",
    "    # ==========================================\n",
    "    # 4. OPTIMIZER & SCHEDULER & LOSS\n",
    "    # ==========================================\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    EPOCHS_OPT = 20 \n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=lr, \n",
    "        steps_per_epoch=len(train_loader_opt),\n",
    "        epochs=EPOCHS_OPT,\n",
    "        pct_start=pct_start\n",
    "    )\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "    scaler_amp = torch.amp.GradScaler('cuda')\n",
    "\n",
    "    # ==========================================\n",
    "    # 5. TRAINING LOOP\n",
    "    # ==========================================\n",
    "    best_f1 = 0.0\n",
    "    \n",
    "    for epoch in range(1, EPOCHS_OPT + 1):\n",
    "        \n",
    "        train_loss, _ = train_one_epoch(\n",
    "            model, train_loader_opt, criterion, optimizer, scheduler, scaler_amp, device\n",
    "        )\n",
    "        \n",
    "        val_loss, val_f1 = validate_one_epoch(\n",
    "            model, val_loader_opt, criterion, device\n",
    "        )\n",
    "\n",
    "        # Pruning  Optuna\n",
    "        trial.report(val_f1, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "            \n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "\n",
    "    return best_f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:04:08.712787Z",
     "iopub.status.busy": "2025-11-29T13:04:08.712484Z",
     "iopub.status.idle": "2025-11-29T13:06:45.650321Z",
     "shell.execute_reply": "2025-11-29T13:06:45.649135Z",
     "shell.execute_reply.started": "2025-11-29T13:04:08.712764Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# STUDY\n",
    "# ==========================================\n",
    "# direction=\"maximize\"  --> maximize F1 score\n",
    "study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner())\n",
    "\n",
    "study.optimize(objective, n_trials=250) \n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"BEST PARAMETERS:\")\n",
    "print(study.best_params)\n",
    "print(f\"BEST F1: {study.best_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T13:06:53.461111Z",
     "iopub.status.busy": "2025-11-29T13:06:53.460340Z",
     "iopub.status.idle": "2025-11-29T13:06:56.261458Z",
     "shell.execute_reply": "2025-11-29T13:06:56.260336Z",
     "shell.execute_reply.started": "2025-11-29T13:06:53.461081Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = \"iframe\" \n",
    "\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances, plot_slice\n",
    "\n",
    "\n",
    "print(\"Importanza degli Iperparametri:\")\n",
    "try:\n",
    "    fig1 = plot_param_importances(study)\n",
    "    fig1.show()\n",
    "except:\n",
    "    print(\"Impossibile mostrare importanza (forse pochi trial completati?)\")\n",
    "\n",
    "# 2. Come  migliorato il modello nel tempo? (Line chart)\n",
    "# Vedrai i pallini salire man mano che Optuna impara\n",
    "print(\"Storia dell'Ottimizzazione:\")\n",
    "fig2 = plot_optimization_history(study)\n",
    "fig2.show()\n",
    "\n",
    "# 3. Dettaglio per ogni parametro (Scatter plot)\n",
    "# Ti fa vedere, ad esempio, che \"tutti i risultati buoni hanno LR basso\"\n",
    "print(\"Dettaglio distribuzione parametri:\")\n",
    "fig3 = plot_slice(study)\n",
    "fig3.show()\n",
    "\n",
    "# 4. Stampa testuale riassuntiva del Best Trial\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"MIGLIOR TRIAL (#{study.best_trial.number})\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Value (F1): {study.best_value:.4f}\")\n",
    "print(\"Params: \")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRrlGMv3pUsh"
   },
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8836751,
     "sourceId": 13869408,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
