{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvKzP/DxMWMcFqOoEtaffV"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "# Required for Imputation/Preprocessing\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, roc_curve, auc\n",
        "from scipy.signal import butter, filtfilt, iirnotch, find_peaks\n",
        "import scipy.io # Required for loading .mat files\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict, Any\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- GLOBAL PARAMETERS ---\n",
        "FS = 500  # Sampling Frequency (Hz)\n",
        "LEAD_II_INDEX = 1\n",
        "BEAT_LENGTH_SAMPLES = 600\n",
        "NUMERICAL_FEATURES = ['age_at_exam', 'weight', 'trainning_load', 'BMI', 'BSA']\n",
        "CATEGORICAL_FEATURES = ['sex', 'sport_classification']\n",
        "TABULAR_INPUT_SIZE = 9 # Calculated as 5 Numerical + 2 One-Hot (sex) + 2 One-Hot (sport_classification)\n",
        "N_SPLITS = 5\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 10\n",
        "# NOTE: Set to 5 to run all folds. Change to 1 for quick execution.\n",
        "NUM_FOLDS_TO_RUN = 5\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "vRUm936XagKE",
        "outputId": "95cfbcd4-54f1-482f-d6b0-b6f166c547d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------------------------\n",
        "# 1. ECG SIGNAL PROCESSING FUNCTIONS\n",
        "# --------------------------------------------------------------------------------\n",
        "\n",
        "def apply_bandpass_filter(data, lowcut=1.0, highcut=40.0, fs=FS, order=5):\n",
        "    \"\"\"Applies a Butterworth bandpass filter.\"\"\"\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    return filtfilt(b, a, data)\n",
        "\n",
        "def notch_filter(data, notch_freq=50.0, fs=FS, Q=30):\n",
        "    \"\"\"Applies a 50Hz Notch filter to remove powerline noise.\"\"\"\n",
        "    nyq = 0.5 * fs\n",
        "    w0 = notch_freq / nyq\n",
        "    b, a = iirnotch(w0, Q)\n",
        "    return filtfilt(b, a, data)\n",
        "\n",
        "def preprocess_signal(ecg: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Applies per-lead Z-score normalization to the already-filtered signal.\"\"\"\n",
        "    ecg = ecg.copy()\n",
        "    for i in range(ecg.shape[1]):\n",
        "        ecg[:, i] = (ecg[:, i] - np.mean(ecg[:, i])) / (np.std(ecg[:, i]) + 1e-6)\n",
        "    return ecg\n",
        "\n",
        "def r_peak_detection_and_segmentation(ecg_12_leads: np.ndarray, fs: int = FS) -> np.ndarray:\n",
        "    \"\"\"Detects R-peaks, segments beats, and returns the average beat morphology.\"\"\"\n",
        "    lead_for_detection = ecg_12_leads[:, LEAD_II_INDEX]\n",
        "\n",
        "    diff_signal = np.diff(lead_for_detection)**2\n",
        "    window_size = int(0.150 * fs)\n",
        "    integrated_signal = np.convolve(diff_signal, np.ones(window_size)/window_size, mode='same')\n",
        "\n",
        "    distance_min = int(0.3 * fs)\n",
        "    peak_threshold = np.max(integrated_signal) * 0.4\n",
        "    r_peaks_idx, _ = find_peaks(integrated_signal, height=peak_threshold, distance=distance_min)\n",
        "\n",
        "    all_beats = []\n",
        "    half_beat = BEAT_LENGTH_SAMPLES // 2\n",
        "\n",
        "    for r_idx in r_peaks_idx:\n",
        "        start_idx = r_idx - half_beat\n",
        "        end_idx = r_idx + half_beat\n",
        "        if start_idx >= 0 and end_idx <= ecg_12_leads.shape[0]:\n",
        "            beat = ecg_12_leads[start_idx:end_idx, :]\n",
        "            all_beats.append(beat)\n",
        "\n",
        "    if not all_beats:\n",
        "        return np.zeros((12, BEAT_LENGTH_SAMPLES), dtype=np.float32)\n",
        "\n",
        "    segmented_ecg = np.array(all_beats, dtype=np.float32)\n",
        "    segmented_ecg = np.transpose(segmented_ecg, (0, 2, 1))\n",
        "\n",
        "    representative_ecg = np.mean(segmented_ecg, axis=0)\n",
        "    return representative_ecg"
      ],
      "metadata": {
        "id": "99VaKBGEagHR"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------------------------\n",
        "# 2. TABULAR FEATURE ENGINEERING AND PROCESSING\n",
        "# --------------------------------------------------------------------------------\n",
        "\n",
        "def tabular_feature_engineering(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Creates BMI and BSA, and cleans initial outliers.\"\"\"\n",
        "    df = df.copy()\n",
        "    df.loc[(df['age_at_exam'] < 0.0) | (df['age_at_exam'] > 100.0), 'age_at_exam'] = np.nan\n",
        "    df.loc[(df['trainning_load'] <= 0.0) | (df['trainning_load'] > 4.0), 'trainning_load'] = np.nan\n",
        "\n",
        "    df['height_m'] = df['height'] / 100.0\n",
        "    df['BMI'] = df['weight'] / (df['height_m']**2)\n",
        "    df['BSA'] = np.sqrt((df['height'] * df['weight']) / 3600.0)\n",
        "\n",
        "    df = df.drop(columns=['height', 'height_m'])\n",
        "    return df\n",
        "\n",
        "class TabularProcessor:\n",
        "    \"\"\"Handles Iterative Imputation, StandardScaler, and OneHotEncoder.\"\"\"\n",
        "    def __init__(self, numerical_features: List[str], categorical_features: List[str]):\n",
        "        self.numerical_features = numerical_features\n",
        "        self.categorical_features = categorical_features\n",
        "        self.imputer = IterativeImputer(max_iter=10, random_state=42)\n",
        "        self.scaler = StandardScaler()\n",
        "        self.encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "\n",
        "    def fit(self, df: pd.DataFrame):\n",
        "        numerical_data_train = df[self.numerical_features].values\n",
        "        self.imputer.fit(numerical_data_train)\n",
        "        imputed_data_train = self.imputer.transform(numerical_data_train)\n",
        "        self.scaler.fit(imputed_data_train)\n",
        "        categorical_data_train = df[self.categorical_features].astype(str).values\n",
        "        self.encoder.fit(categorical_data_train)\n",
        "\n",
        "    def transform(self, df: pd.DataFrame) -> np.ndarray:\n",
        "        numerical_data = df[self.numerical_features].values\n",
        "        imputed_data = self.imputer.transform(numerical_data)\n",
        "        scaled_data = self.scaler.transform(imputed_data)\n",
        "        categorical_data = df[self.categorical_features].astype(str).values\n",
        "        encoded_data = self.encoder.transform(categorical_data)\n",
        "        return np.concatenate([scaled_data, encoded_data], axis=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "EmC8NhayagEq"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------------------------\n",
        "# 3. PYTORCH DATASET AND MODEL DEFINITIONS\n",
        "# --------------------------------------------------------------------------------\n",
        "\n",
        "class ECGDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset to load ECG and tabular data.\"\"\"\n",
        "    def __init__(self, ecg_data: List[np.ndarray], tabular_data: pd.DataFrame, labels: np.ndarray):\n",
        "        self.ecg_data = ecg_data\n",
        "        self.tabular_data = tabular_data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        # ECG signal is already filtered and baseline corrected from the loading step (Block 1)\n",
        "        raw_ecg = self.ecg_data[idx]\n",
        "        cleaned_ecg = preprocess_signal(raw_ecg)\n",
        "        segmented_ecg = r_peak_detection_and_segmentation(cleaned_ecg)\n",
        "        ecg_tensor = torch.from_numpy(segmented_ecg).float()\n",
        "\n",
        "        tabular_row = self.tabular_data.iloc[idx].values\n",
        "        tabular_tensor = torch.from_numpy(tabular_row).float()\n",
        "\n",
        "        label_tensor = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "\n",
        "        return {\n",
        "            'ecg': ecg_tensor, 'tabular': tabular_tensor, 'label': label_tensor\n",
        "        }\n",
        "\n",
        "class ThreeBranchCNNModel(nn.Module):\n",
        "    \"\"\"CNN model with fixed feature size (4640).\"\"\"\n",
        "    def __init__(self, tabular_input_size, num_classes=1):\n",
        "        super(ThreeBranchCNNModel, self).__init__()\n",
        "\n",
        "        conv_block = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=6, out_channels=32, kernel_size=16, stride=2, padding=7), nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=4, stride=2),\n",
        "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=8, stride=2, padding=3), nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=4, stride=2),\n",
        "        )\n",
        "        self.ecg_frontal_branch = conv_block\n",
        "        self.ecg_precordial_branch = conv_block\n",
        "\n",
        "        output_sequence_length = 36\n",
        "        feature_size_per_branch = 64 * output_sequence_length\n",
        "\n",
        "        self.tabular_branch = nn.Sequential(\n",
        "            nn.Linear(tabular_input_size, 64), nn.ReLU(), nn.Dropout(0.3), nn.Linear(64, 32)\n",
        "        )\n",
        "\n",
        "        total_combined_size = feature_size_per_branch * 2 + 32 # 4640\n",
        "\n",
        "        self.classifier_head = nn.Sequential(\n",
        "            nn.Linear(total_combined_size, 256), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes), nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, ecg_tensor, tabular_tensor):\n",
        "        ecg_frontal = ecg_tensor[:, :6, :]\n",
        "        ecg_precordial = ecg_tensor[:, 6:, :]\n",
        "\n",
        "        feat_frontal = torch.flatten(self.ecg_frontal_branch(ecg_frontal), 1)\n",
        "        feat_precordial = torch.flatten(self.ecg_precordial_branch(ecg_precordial), 1)\n",
        "        feat_tabular = self.tabular_branch(tabular_tensor)\n",
        "\n",
        "        combined_features = torch.cat((feat_frontal, feat_precordial, feat_tabular), dim=1)\n",
        "\n",
        "        return self.classifier_head(combined_features)\n"
      ],
      "metadata": {
        "id": "AdUnsKydaf9a"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# 4. METRICS AND PLOTTING FUNCTIONS\n",
        "# --------------------------------------------------------------------------------\n",
        "\n",
        "def compute_metrics(y_true, y_pred_proba):\n",
        "    \"\"\"Computes AUC, Accuracy, Sensitivity, and Specificity.\"\"\"\n",
        "    if len(np.unique(y_true)) < 2:\n",
        "        auc_score = 0.5\n",
        "        fpr, tpr = np.array([0.0, 1.0]), np.array([0.0, 1.0])\n",
        "    else:\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
        "        auc_score = auc(fpr, tpr)\n",
        "\n",
        "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "    return {\n",
        "        'AUC': auc_score, 'Accuracy': accuracy,\n",
        "        'Sensitivity': sensitivity, 'Specificity': specificity,\n",
        "        'FPR': fpr, 'TPR': tpr, 'CM': cm, 'Y_PRED_BINARY': y_pred\n",
        "    }\n",
        "\n",
        "def plot_confusion_matrix(cm, fold_num, file_path):\n",
        "    \"\"\"Plots and saves the Confusion Matrix for a specific fold.\"\"\"\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title(f'Confusion Matrix - Fold {fold_num}')\n",
        "    plt.colorbar(fraction=0.046, pad=0.04)\n",
        "    tick_marks = np.arange(2)\n",
        "    plt.xticks(tick_marks, ['Negative (0)', 'Positive (1)'])\n",
        "    plt.yticks(tick_marks, ['Negative (0)', 'Positive (1)'])\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            plt.text(j, i, format(cm[i, j], 'd'),\n",
        "                     ha=\"center\", va=\"center\", fontsize=16,\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(file_path)\n",
        "    plt.close()\n",
        "\n",
        "def plot_aggregate_roc(all_fprs, all_tprs, all_aucs, file_path):\n",
        "    \"\"\"Plots the aggregate ROC curve across all folds.\"\"\"\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    mean_fpr = np.linspace(0, 1, 100)\n",
        "    tprs_interp = []\n",
        "\n",
        "    for i in range(len(all_fprs)):\n",
        "        tprs_interp.append(np.interp(mean_fpr, all_fprs[i], all_tprs[i]))\n",
        "        plt.plot(all_fprs[i], all_tprs[i], alpha=0.3, label=f'ROC Fold {i+1} (AUC = {all_aucs[i]:.2f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Chance', alpha=.8)\n",
        "\n",
        "    mean_tpr = np.mean(tprs_interp, axis=0)\n",
        "    mean_tpr[0] = 0.0\n",
        "    mean_tpr[-1] = 1.0\n",
        "    mean_auc = auc(mean_fpr, mean_tpr)\n",
        "    std_auc = np.std(all_aucs)\n",
        "    std_tpr = np.std(tprs_interp, axis=0)\n",
        "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "\n",
        "    plt.plot(mean_fpr, mean_tpr, color='blue',\n",
        "             label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
        "             lw=2, alpha=.8)\n",
        "\n",
        "    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='skyblue', alpha=.2,\n",
        "                     label=r'$\\pm$ 1 standard dev.')\n",
        "\n",
        "    plt.xlim([-0.01, 1.01])\n",
        "    plt.ylim([-0.01, 1.01])\n",
        "    plt.xlabel('False Positive Rate (FPR)')\n",
        "    plt.ylabel('True Positive Rate (TPR)')\n",
        "    plt.title('Aggregate Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\", fontsize='small')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(file_path)\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "NY25hpKLa4BK"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"--- Starting Block 1: Data Loading and Preparation ---\")\n",
        "try:\n",
        "    # --- ASSUMED GOOGLE DRIVE PATHS (Required for .mat files) ---\n",
        "    ECG_folder_1batch = \"/content/drive/MyDrive/WP_02_data/1_batch_extracted\"\n",
        "    ECG_folder_2batch = \"/content/drive/MyDrive/WP_02_data/2_batch_extracted\"\n",
        "\n",
        "    # Load Tabular Data from accessible CSV files\n",
        "    tabular_data_1 = pd.read_excel(\"/content/drive/MyDrive/WP_02_data/VALETUDO_database_1st_batch_en_all_info.xlsx\")\n",
        "    tabular_data_2 = pd.read_excel(\"/content/drive/MyDrive/WP_02_data/VALETUDO_database_2nd_batch_en_all_info.xlsx\")\n",
        "\n",
        "    # Concatenate and sort tabular data\n",
        "    tabular_df = pd.concat([tabular_data_1, tabular_data_2], ignore_index=True)\n",
        "    tabular_df = tabular_df.sort_values(by=\"ECG_patient_id\").reset_index(drop=True)\n",
        "\n",
        "    # --- Load ECG File Paths and Sort ---\n",
        "    ECGs_1 = [f for f in os.listdir(ECG_folder_1batch) if f.endswith(\".mat\")]\n",
        "    ECGs_2 = [f for f in os.listdir(ECG_folder_2batch) if f.endswith(\".mat\")]\n",
        "\n",
        "    def extract_patient_id(filename):\n",
        "        return int(filename.split(\".\")[0])\n",
        "\n",
        "    ECGs_1.sort(key=extract_patient_id)\n",
        "    ECGs_2.sort(key=extract_patient_id)\n",
        "\n",
        "    # NOTE: The list of ECG files should match the number of subjects in the tabular data.\n",
        "    if (len(ECGs_1) + len(ECGs_2)) != len(tabular_df):\n",
        "        print(\"⚠️ Warning: Number of ECG files does not match tabular entries. Check file consistency.\")\n",
        "        # Attempt to proceed using the tabular size as the ground truth\n",
        "        N_SUBJECTS = len(tabular_df)\n",
        "    else:\n",
        "        N_SUBJECTS = len(tabular_df)\n",
        "\n",
        "    # --- Load ECG Signals and Apply Filtering ---\n",
        "    # The expected shape is 5000 samples, 12 leads\n",
        "    raw_ecg_list = []\n",
        "\n",
        "    # Load Batch 1\n",
        "    for ecg_path in tqdm(ECGs_1, desc=\"Loading ECG Batch 1\"):\n",
        "        filepath = os.path.join(ECG_folder_1batch, ecg_path)\n",
        "        matdata = scipy.io.loadmat(filepath)\n",
        "        ecg = matdata['val'].T # ECGs typically come in (12, 5000) or (5000, 12). Transpose if needed. Assume (Samples, Leads) for consistency.\n",
        "        if ecg.shape[0] != 5000: ecg = ecg.T # Ensure (5000, 12)\n",
        "\n",
        "        # Apply filtering and baseline correction during loading\n",
        "        filtered_ecg = np.empty_like(ecg)\n",
        "        for i in range(12):\n",
        "            lead_data = ecg[:, i]\n",
        "            lead_data = lead_data - np.mean(lead_data) # Baseline Correction\n",
        "            lead_data = apply_bandpass_filter(lead_data)\n",
        "            lead_data = notch_filter(lead_data)\n",
        "            filtered_ecg[:, i] = lead_data\n",
        "        raw_ecg_list.append(filtered_ecg.astype(np.float32))\n",
        "\n",
        "    # Load Batch 2\n",
        "    for ecg_path in tqdm(ECGs_2, desc=\"Loading ECG Batch 2\"):\n",
        "        filepath = os.path.join(ECG_folder_2batch, ecg_path)\n",
        "        matdata = scipy.io.loadmat(filepath)\n",
        "        ecg = matdata['val'].T\n",
        "        if ecg.shape[0] != 5000: ecg = ecg.T\n",
        "\n",
        "        # Apply filtering and baseline correction during loading\n",
        "        filtered_ecg = np.empty_like(ecg)\n",
        "        for i in range(12):\n",
        "            lead_data = ecg[:, i]\n",
        "            lead_data = lead_data - np.mean(lead_data) # Baseline Correction\n",
        "            lead_data = apply_bandpass_filter(lead_data)\n",
        "            lead_data = notch_filter(lead_data)\n",
        "            filtered_ecg[:, i] = lead_data\n",
        "        raw_ecg_list.append(filtered_ecg.astype(np.float32))\n",
        "\n",
        "    print(f\"Successfully loaded {N_SUBJECTS} subjects.\")\n",
        "    print(f\"First ECG signal shape: {raw_ecg_list[0].shape}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ FATAL ERROR: Data Loading Failed ❌\")\n",
        "    print(\"The script failed to load data, likely because the Google Drive paths were not mounted or the files were inaccessible.\")\n",
        "    print(f\"Error details: {e}\")\n",
        "    # Exit or raise error to stop further execution if real data is mandatory\n",
        "    raise RuntimeError(\"Failed to load real ECG data from disk. Execution stopped.\")\n",
        "\n",
        "\n",
        "# 1. Feature Engineering (Applied once)\n",
        "labels = tabular_df['sport_ability'].values\n",
        "processed_tabular_df = tabular_feature_engineering(tabular_df)\n",
        "\n",
        "# 2. Tabular Preprocessing to get input size\n",
        "temp_processor = TabularProcessor(NUMERICAL_FEATURES, CATEGORICAL_FEATURES)\n",
        "temp_processor.fit(processed_tabular_df)\n",
        "TABULAR_INPUT_SIZE = temp_processor.transform(processed_tabular_df.head(1)).shape[1]\n",
        "\n",
        "print(f\"Block 1: Data preparation complete. TABULAR_INPUT_SIZE: {TABULAR_INPUT_SIZE}\")"
      ],
      "metadata": {
        "id": "ij84HnkVa9Ai",
        "outputId": "9890da8a-6711-4677-d198-166f3fdeff30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Block 1: Data Loading and Preparation ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading ECG Batch 1: 100%|██████████| 191/191 [00:07<00:00, 23.88it/s]\n",
            "Loading ECG Batch 2: 100%|██████████| 335/335 [00:10<00:00, 32.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded 526 subjects.\n",
            "First ECG signal shape: (5000, 12)\n",
            "Block 1: Data preparation complete. TABULAR_INPUT_SIZE: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------------------------\n",
        "# BLOCK 2: K-FOLD EXECUTION WITH TRAINING AND METRICS\n",
        "# --------------------------------------------------------------------------------\n",
        "print(\"\\n--- Starting Block 2: K-Fold Cross-Validation ---\")\n",
        "print(f\"Running {NUM_FOLDS_TO_RUN} Fold(s), {NUM_EPOCHS} Epochs.\")\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
        "indices = np.arange(N_SUBJECTS)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Stratified K-Fold with {N_SPLITS} splits initialized. Using device: {device}\")\n",
        "\n",
        "# Lists to store metrics across folds\n",
        "all_fprs = []\n",
        "all_tprs = []\n",
        "all_aucs = []\n",
        "fold_metrics = []\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(skf.split(indices, labels)):\n",
        "    if fold >= NUM_FOLDS_TO_RUN:\n",
        "        break\n",
        "\n",
        "    print(f\"\\n--- Starting Fold {fold+1}/{N_SPLITS} ---\")\n",
        "\n",
        "    # --- A. SPLIT DATA ---\n",
        "    train_df_raw = processed_tabular_df.iloc[train_index].reset_index(drop=True)\n",
        "    val_df_raw = processed_tabular_df.iloc[val_index].reset_index(drop=True)\n",
        "    train_labels = labels[train_index]\n",
        "    val_labels = labels[val_index]\n",
        "    train_ecg_list = [raw_ecg_list[i] for i in train_index]\n",
        "    val_ecg_list = [raw_ecg_list[i] for i in val_index]\n",
        "\n",
        "    # --- B. TABULAR PREPROCESSING (Fit only on training data) ---\n",
        "    tabular_processor = TabularProcessor(NUMERICAL_FEATURES, CATEGORICAL_FEATURES)\n",
        "    tabular_processor.fit(train_df_raw)\n",
        "    train_tab_processed_arr = tabular_processor.transform(train_df_raw)\n",
        "    val_tab_processed_arr = tabular_processor.transform(val_df_raw)\n",
        "\n",
        "    train_tab_processed_df = pd.DataFrame(train_tab_processed_arr)\n",
        "    val_tab_processed_df = pd.DataFrame(val_tab_processed_arr)\n",
        "\n",
        "    # --- C. DATASET AND DATALOADER CREATION ---\n",
        "    train_dataset = ECGDataset(\n",
        "        ecg_data=train_ecg_list, tabular_data=train_tab_processed_df, labels=train_labels\n",
        "    )\n",
        "    val_dataset = ECGDataset(\n",
        "        ecg_data=val_ecg_list, tabular_data=val_tab_processed_df, labels=val_labels\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # --- D. MODEL TRAINING SETUP ---\n",
        "    model = ThreeBranchCNNModel(tabular_input_size=TABULAR_INPUT_SIZE).to(device)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    print(f\"  Train size: {len(train_dataset)}, Validation size: {len(val_dataset)}\")\n",
        "\n",
        "    # --- E. TRAINING LOOP ---\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            ecg_data = batch['ecg'].to(device)\n",
        "            tabular_data = batch['tabular'].to(device)\n",
        "            labels_tensor = batch['label'].to(device).unsqueeze(1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(ecg_data, tabular_data)\n",
        "            loss = criterion(outputs, labels_tensor)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * ecg_data.size(0)\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_dataset)\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_y_true = []\n",
        "        val_y_pred_proba = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                ecg_data = batch['ecg'].to(device)\n",
        "                tabular_data = batch['tabular'].to(device)\n",
        "                labels_tensor = batch['label'].to(device).unsqueeze(1)\n",
        "\n",
        "                outputs = model(ecg_data, tabular_data)\n",
        "                loss = criterion(outputs, labels_tensor)\n",
        "                val_loss += loss.item() * ecg_data.size(0)\n",
        "\n",
        "                # Store predictions and true labels for metrics\n",
        "                val_y_true.extend(labels_tensor.cpu().numpy().flatten())\n",
        "                val_y_pred_proba.extend(outputs.cpu().numpy().flatten())\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_dataset)\n",
        "\n",
        "        # --- F. METRICS CALCULATION AND REPORTING (Only at the end of the last epoch) ---\n",
        "        if epoch == NUM_EPOCHS - 1:\n",
        "            val_metrics = compute_metrics(np.array(val_y_true), np.array(val_y_pred_proba))\n",
        "\n",
        "            print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "            print(f\"    Validation Metrics (Final Epoch):\")\n",
        "            print(f\"      AUC: {val_metrics['AUC']:.4f}\")\n",
        "            print(f\"      Accuracy: {val_metrics['Accuracy']:.4f}\")\n",
        "            print(f\"      Sensitivity: {val_metrics['Sensitivity']:.4f}\")\n",
        "            print(f\"      Specificity: {val_metrics['Specificity']:.4f}\")\n",
        "\n",
        "            # Store for aggregate plots\n",
        "            all_fprs.append(val_metrics['FPR'])\n",
        "            all_tprs.append(val_metrics['TPR'])\n",
        "            all_aucs.append(val_metrics['AUC'])\n",
        "            fold_metrics.append({\n",
        "                'Fold': fold + 1,\n",
        "                'AUC': val_metrics['AUC'],\n",
        "                'Accuracy': val_metrics['Accuracy'],\n",
        "                'Sensitivity': val_metrics['Sensitivity'],\n",
        "                'Specificity': val_metrics['Specificity']\n",
        "            })\n",
        "\n",
        "            # Save Confusion Matrix\n",
        "            cm_filename = f\"confusion_matrix_fold_{fold+1}.png\"\n",
        "            plot_confusion_matrix(val_metrics['CM'], fold+1, cm_filename)\n",
        "            print(f\"  Confusion Matrix saved as {cm_filename}\")\n",
        "\n",
        "        else:\n",
        "            print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    print(f\"--- Fold {fold+1} Completed ({NUM_EPOCHS} Epochs) ---\")\n",
        "\n",
        "# --- G. FINAL AGGREGATE METRICS & PLOT (if running multiple folds) ---\n",
        "if NUM_FOLDS_TO_RUN > 1:\n",
        "    agg_df = pd.DataFrame(fold_metrics)\n",
        "    mean_metrics = agg_df.drop(columns=['Fold']).mean().to_dict()\n",
        "    std_metrics = agg_df.drop(columns=['Fold']).std().to_dict()\n",
        "\n",
        "    print(\"\\n--- Aggregate Cross-Validation Results ---\")\n",
        "    print(f\"Mean AUC: {mean_metrics['AUC']:.4f} \\u00B1 {std_metrics.get('AUC', 0):.4f}\")\n",
        "    print(f\"Mean Accuracy: {mean_metrics['Accuracy']:.4f} \\u00B1 {std_metrics.get('Accuracy', 0):.4f}\")\n",
        "    print(f\"Mean Sensitivity: {mean_metrics['Sensitivity']:.4f} \\u00B1 {std_metrics.get('Sensitivity', 0):.4f}\")\n",
        "    print(f\"Mean Specificity: {mean_metrics['Specificity']:.4f} \\u00B1 {std_metrics.get('Specificity', 0):.4f}\")\n",
        "\n",
        "    roc_filename = \"aggregate_roc_curve.png\"\n",
        "    plot_aggregate_roc(all_fprs, all_tprs, all_aucs, roc_filename)\n",
        "    print(f\"Aggregate ROC Curve saved as {roc_filename}\")\n",
        "\n",
        "print(\"\\n--- Block 2 Execution Finished ---\")"
      ],
      "metadata": {
        "id": "XRDyxFrRcAVU",
        "outputId": "1ad6c311-04c5-4bb0-90b4-c3a1a2081281",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Block 2: K-Fold Cross-Validation ---\n",
            "Running 5 Fold(s), 10 Epochs.\n",
            "Stratified K-Fold with 5 splits initialized. Using device: cpu\n",
            "\n",
            "--- Starting Fold 1/5 ---\n",
            "  Train size: 420, Validation size: 106\n",
            "  Epoch 1/10, Train Loss: 0.6435, Val Loss: 0.6117\n",
            "  Epoch 2/10, Train Loss: 0.5830, Val Loss: 0.5824\n",
            "  Epoch 3/10, Train Loss: 0.5591, Val Loss: 0.5581\n",
            "  Epoch 4/10, Train Loss: 0.5289, Val Loss: 0.5278\n",
            "  Epoch 5/10, Train Loss: 0.4910, Val Loss: 0.5409\n",
            "  Epoch 6/10, Train Loss: 0.4284, Val Loss: 0.5675\n",
            "  Epoch 7/10, Train Loss: 0.4700, Val Loss: 0.6241\n",
            "  Epoch 8/10, Train Loss: 0.4044, Val Loss: 0.5259\n",
            "  Epoch 9/10, Train Loss: 0.3585, Val Loss: 0.5708\n",
            "  Epoch 10/10, Train Loss: 0.3310, Val Loss: 0.5846\n",
            "    Validation Metrics (Final Epoch):\n",
            "      AUC: 0.7639\n",
            "      Accuracy: 0.7264\n",
            "      Sensitivity: 0.9167\n",
            "      Specificity: 0.3235\n",
            "  Confusion Matrix saved as confusion_matrix_fold_1.png\n",
            "--- Fold 1 Completed (10 Epochs) ---\n",
            "\n",
            "--- Starting Fold 2/5 ---\n",
            "  Train size: 421, Validation size: 105\n",
            "  Epoch 1/10, Train Loss: 0.7034, Val Loss: 0.6523\n",
            "  Epoch 2/10, Train Loss: 0.6082, Val Loss: 0.6004\n",
            "  Epoch 3/10, Train Loss: 0.5868, Val Loss: 0.5861\n",
            "  Epoch 4/10, Train Loss: 0.5516, Val Loss: 0.5734\n",
            "  Epoch 5/10, Train Loss: 0.5280, Val Loss: 0.5646\n",
            "  Epoch 6/10, Train Loss: 0.4998, Val Loss: 0.5538\n",
            "  Epoch 7/10, Train Loss: 0.4795, Val Loss: 0.5349\n",
            "  Epoch 8/10, Train Loss: 0.4557, Val Loss: 0.5281\n",
            "  Epoch 9/10, Train Loss: 0.4193, Val Loss: 0.5512\n",
            "  Epoch 10/10, Train Loss: 0.4086, Val Loss: 0.5166\n",
            "    Validation Metrics (Final Epoch):\n",
            "      AUC: 0.8237\n",
            "      Accuracy: 0.8286\n",
            "      Sensitivity: 0.9444\n",
            "      Specificity: 0.5758\n",
            "  Confusion Matrix saved as confusion_matrix_fold_2.png\n",
            "--- Fold 2 Completed (10 Epochs) ---\n",
            "\n",
            "--- Starting Fold 3/5 ---\n",
            "  Train size: 421, Validation size: 105\n",
            "  Epoch 1/10, Train Loss: 0.6387, Val Loss: 0.6182\n",
            "  Epoch 2/10, Train Loss: 0.5903, Val Loss: 0.5790\n",
            "  Epoch 3/10, Train Loss: 0.5359, Val Loss: 0.5852\n",
            "  Epoch 4/10, Train Loss: 0.5260, Val Loss: 0.6081\n",
            "  Epoch 5/10, Train Loss: 0.4824, Val Loss: 0.5783\n",
            "  Epoch 6/10, Train Loss: 0.4494, Val Loss: 0.6088\n",
            "  Epoch 7/10, Train Loss: 0.4428, Val Loss: 0.6144\n",
            "  Epoch 8/10, Train Loss: 0.3931, Val Loss: 0.6852\n",
            "  Epoch 9/10, Train Loss: 0.3645, Val Loss: 0.7664\n",
            "  Epoch 10/10, Train Loss: 0.3520, Val Loss: 0.7150\n",
            "    Validation Metrics (Final Epoch):\n",
            "      AUC: 0.6684\n",
            "      Accuracy: 0.6952\n",
            "      Sensitivity: 0.8194\n",
            "      Specificity: 0.4242\n",
            "  Confusion Matrix saved as confusion_matrix_fold_3.png\n",
            "--- Fold 3 Completed (10 Epochs) ---\n",
            "\n",
            "--- Starting Fold 4/5 ---\n",
            "  Train size: 421, Validation size: 105\n",
            "  Epoch 1/10, Train Loss: 0.6329, Val Loss: 0.6047\n",
            "  Epoch 2/10, Train Loss: 0.5690, Val Loss: 0.5766\n",
            "  Epoch 3/10, Train Loss: 0.5205, Val Loss: 0.5565\n",
            "  Epoch 4/10, Train Loss: 0.5064, Val Loss: 0.5415\n",
            "  Epoch 5/10, Train Loss: 0.4424, Val Loss: 0.5599\n",
            "  Epoch 6/10, Train Loss: 0.4074, Val Loss: 0.6083\n",
            "  Epoch 7/10, Train Loss: 0.3744, Val Loss: 0.6355\n",
            "  Epoch 8/10, Train Loss: 0.3921, Val Loss: 0.7274\n",
            "  Epoch 9/10, Train Loss: 0.3330, Val Loss: 0.6323\n",
            "  Epoch 10/10, Train Loss: 0.3309, Val Loss: 0.7441\n",
            "    Validation Metrics (Final Epoch):\n",
            "      AUC: 0.6688\n",
            "      Accuracy: 0.7143\n",
            "      Sensitivity: 0.8889\n",
            "      Specificity: 0.3333\n",
            "  Confusion Matrix saved as confusion_matrix_fold_4.png\n",
            "--- Fold 4 Completed (10 Epochs) ---\n",
            "\n",
            "--- Starting Fold 5/5 ---\n",
            "  Train size: 421, Validation size: 105\n",
            "  Epoch 1/10, Train Loss: 0.6154, Val Loss: 0.5962\n",
            "  Epoch 2/10, Train Loss: 0.5567, Val Loss: 0.5808\n",
            "  Epoch 3/10, Train Loss: 0.5188, Val Loss: 0.5918\n",
            "  Epoch 4/10, Train Loss: 0.5061, Val Loss: 0.5850\n",
            "  Epoch 5/10, Train Loss: 0.4402, Val Loss: 0.6371\n",
            "  Epoch 6/10, Train Loss: 0.4099, Val Loss: 0.7280\n",
            "  Epoch 7/10, Train Loss: 0.4058, Val Loss: 0.6906\n",
            "  Epoch 8/10, Train Loss: 0.3380, Val Loss: 0.7724\n",
            "  Epoch 9/10, Train Loss: 0.2794, Val Loss: 0.8768\n",
            "  Epoch 10/10, Train Loss: 0.2825, Val Loss: 0.8454\n",
            "    Validation Metrics (Final Epoch):\n",
            "      AUC: 0.6221\n",
            "      Accuracy: 0.6571\n",
            "      Sensitivity: 0.7778\n",
            "      Specificity: 0.3939\n",
            "  Confusion Matrix saved as confusion_matrix_fold_5.png\n",
            "--- Fold 5 Completed (10 Epochs) ---\n",
            "\n",
            "--- Aggregate Cross-Validation Results ---\n",
            "Mean AUC: 0.7093 ± 0.0821\n",
            "Mean Accuracy: 0.7243 ± 0.0639\n",
            "Mean Sensitivity: 0.8694 ± 0.0692\n",
            "Mean Specificity: 0.4102 ± 0.1016\n",
            "Aggregate ROC Curve saved as aggregate_roc_curve.png\n",
            "\n",
            "--- Block 2 Execution Finished ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xYl_n4-zbVYA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}